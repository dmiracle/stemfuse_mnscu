<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Resampling Methods</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

  <link href="inc/metroStyle.css" type="text/css" rel="stylesheet">
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.js" type="text/javascript"></script>
  <script src="inc/metroScript.js" type="text/javascript"></script>

  <!--LaTex Includes
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.4.1/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">
  -->

  <style>

    .warning {
    visibility: hidden;
  }
  #leftSideBar {
      background-color: #003C66;
      background-repeat: no-repeat;
      background-position: top center;
      background-size: 100%;
      min-height: 100%;
      width: 200px;
      float: left;
      padding-top: 10px;
      padding-right: 1%;
      padding-left: 1%;
      font-size: 1em;
      line-height: 100%;
      color: white;
  }
  
  #leftSideBar a {
      color: white;
      text-decoration: none;
  }
  
  #leftSideBar li {
      margin:0 0 1em 0;
  }
  
  </style>
</head>

<body>
  <p style="text-align: center;"><a name="top"></a></p>
  <div id="pageContainer">
    <div class="toTop"><a href="#top">Top of Page &gt;&gt;</a></div>
    <div id="leftSideBar">
      <img src="images/ITCoE_white200px.png">
      <!---
  <h2 style="color:white;">Sections</h2>
-->
      <hr style="color: white;">
      <ul>
        <li>
          <h2 style="color: white;">Sections</h2>
        </li>
      </ul>
      <hr style="color: white;">
      <ul>
        <!--- Place sidebar links here like this:
    <li><a href="#learning-objectives">Learning Objectives</a></li>
-->
        <li><a href="#instructor-overview-and-objective">Instructor Overview and Objective</a></li>
        <li><a href="#table-of-contents">Table of Contents</a></li>
        <li><a href="#sets--overview">1. Sets - Overview</a></li>
        <li><a href="#programming">2. Programming</a></li>
        <li><a href="#relation">3. Relation</a></li>
        <li><a href="#function">4. Function</a></li>
        <h2 id="introduction-to-bootstrap-sampling">Introduction to Bootstrap Sampling</h2>
        <h2 id="theoretical-explanation-of-bootstrap-sampling">Theoretical Explanation of Bootstrap Sampling</h2>
        <h2 id="bootstrap-confidence-interval">Bootstrap Confidence Interval</h2>
        <h2 id="confidence-interval-based-on-bootstrap-percentiles">Confidence Interval based on Bootstrap Percentiles</h2>
        <h2 id="use-r-package-boot-to-generate-bootstrap-sample-statistics">Use R package boot to generate bootstrap sample statistics</h2>
        <!--- end sidebar -->
      </ul>
      <div id="printDiv" style="text-align: center;"><input onclick="window.print(); return false;" alt="Print" src="https://metrostate.learn.minnstate.edu/shared/Admin/Templates/Templates/../../../images/icons/print.png"
          width="32" type="image" height="32" /></div>
      <hr style="color: white;">
      <img src="images/mnstate_white200px.png">
    </div>
    <div id="mainContent">
      <div class="warning"><span style="color: #cf2a27;"><strong>Do not delete... If you delete into this warning
            block, Control + Z to Undo... Do not delete</strong></span></div>
<!-- PASTE HTML HERE-->

<h1 id="module-resampling-methods">Module Resampling Methods</h1>
<p>Authors: David Jacobson, Ph.D., and Wei Wei, Ph.D.</p>
<p>Competency Area: Big Data</p>
<p><strong>Learning objectives</strong>: After this module, students will be able to</p>
<ul>
<li><p>Describe and explain the bootstrap resampling method</p></li>
<li><p>Generate bootstrap samples using R</p></li>
<li><p>Solve related problems using bootstrap resampling method</p></li>
<li><p>Describe and explain the cross-validation assessment method</p></li>
<li><p>Create and use a function for cross-validating a model’s R-square statistic using <em>k</em>-fold cross-validation</p></li>
<li><p>Use the <strong>cv.glm</strong> function in the <strong>boot</strong> package in R for performing cross-validation for generalized linear models</p></li>
<li><p>Understand the difference between the raw cross-validation error and the adjusted cross-validation error</p></li>
</ul>
<p><strong>Resources</strong></p>
<p><a href="https://www.youtube.com/watch?v=_nhgHjdLE-I">https://www.youtube.com/watch?v=_nhgHjdLE-I</a></p>
<p><a href="http://www.stat.umn.edu/geyer/3701/notes/bootstrap.html">http://www.stat.umn.edu/geyer/3701/notes/bootstrap.html</a></p>
<p><a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/state.html%20">https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/state.html</a></p>
<p><a href="https://vincentarelbundock.github.io/Rdatasets/doc/boot/nuclear.html">https://vincentarelbundock.github.io/Rdatasets/doc/boot/nuclear.html</a></p>
<p><a href="https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm">https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm</a></p>

<h3 id="introduction">Introduction</h3>
<p>Building a model can be a never ending process in which we constantly improve the model by adding interactions, taking away variables, doing transformations and so on. However, at some point we need to confirm that we have the best model at the time, or even a good model. That leads to the question: How do we judge the quality of a model? In almost all cases the answer has to be in relation to other models. This could be an analysis of residuals, the results of an ANOVA test, a Wald test, drop-in deviance, and the AIC or BIC score. In this module, we consider bootstrapping and cross-validation error.</p>
<p>Sometimes, for one reason or another, there is not a good analytic solution to a problem and another tactic is needed. This is especially true for measuring uncertainty for confidence intervals. To overcome this, Bradley Efron introduced the bootstrap in 1979. Since then the bootstrap has grown to revolutionize modern statistics and is indispensable.</p>
<p>Residual diagnostics and model tests such as ANOVA and AIC are a bit old fashioned and came along before modern computing horsepower. The preferred method to assess model quality – at least by most data scientists – is cross-validation, sometimes called <em>k</em>-fold cross-validation. The data is broken into <em>k</em> (usually five or ten) non-overlapping sections. Then a model is fitted on <span class="math inline">\(k - 1\)</span> sections of the data, which is then used to make predictions based on the <em>k</em>th section. This is repeated <em>k</em> times until every section has been held out for testing once and included in model fitting <span class="math inline">\(k - 1\)</span> times. Cross-validation provides a measure of the predictive accuracy of a model, which is largely considered a good means of assessing model quality.</p>
<h2 id="introduction-to-bootstrap-sampling">Introduction to Bootstrap Sampling</h2>
<p>The bootstrap is a widely applicable and extremely powerful sampling method. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. The power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.</p>
<p>Bootstrap resampling is to use one random sample as a population, and repeatedly draw samples with the same size from that population. Ideally, we would like to sample repeatedly from the population to create a sampling distribution. In practice, it is not feasible to know the population. Instead of actually making many copies of the sample and sampling from the population, we use a sampling technique that is equivalent: we sample with replacement from the original sample. <strong>Sampling with replacement</strong> means that once an individual is selected for the sample, that individual is still available to be selected again. Each sample selected in this way, with replacement from the original sample, is called a <strong>bootstrap sample</strong>.</p>
<p>Figure 1 illustrates the bootstrap sampling. Suppose the four shapes (triangle, rectangle, diamond, and pentagon) form the original sample. Three possible bootstrap samples are {triangle, rectangle, triangle, pentagon}, {pentagon, diamond, pentagon, pentagon}, and {triangle, rectangle, diamond, diamond}. Each bootstrap sample contains the same number of shapes as the original sample. Each shape can be reselected in each sample.</p>
<p><img src="media/media/figure1.png" /></p>
<p>Figure 1. Illustration of bootstrap samples</p>
<p>For each bootstrap sample, we can compute the statistic of interest, giving us a <strong>bootstrap statistic</strong>. Example 1 illustrates how to obtain a bootstrap sample, and further how to obtain bootstrap distribution of bootstrap statistic.</p>
<p>Example 1. In a data science course, there are six students with their final grades being {70,81,85,92,95, 63}. Generate a bootstrap sample using R.</p>
<p>The R code is</p>
<p><img src="media/media/image1.png" style="width:3.46875in;height:0.5in" /></p>
<p>The first line of the code is to generate a vector of the data for the students’ grades. The second line is to generate a bootstrap sample, and name it as “bootsample”. The function sample() is to generate a random bootstrap sample based on the original sample, grades. The argument “replace=TRUE” is to generate a sample with replacement. The third line “bootsample” is to show the sample elements.</p>
<p>In this example, the bootstrap sample is {63,63,85,81,95,81}. As this is a random process, each time you run the code, you will get a different sample.</p>
<p>Use the R function mean() to calculate the mean of the bootstrap sample.</p>
<p><img src="media/media/image2.png" style="width:1.57292in;height:0.17708in" /></p>
<p>The mean of the above bootstrap sample is 78.</p>
<p>If we repeat this process multiple times and obtain multiple bootstrap sample means, then we can form a <strong>bootstrap distribution</strong>. In example 1, if we run the bootstrap sampling process multiple times, we will get multiple bootstrap sample means. Again, this is a random process, so the bootstrap sample means will be random. Example 2 shows how to generate bootstrap distribution using R.</p>
<p>Example 2. Based on the six students’ grades in example 1, generate 20 bootstrap samples, find the bootstrap sample means, and form a bootstrap distribution using a histogram.</p>
<p><img src="media/media/image3.png" style="width:3.73958in;height:1.26042in" /></p>
<p>The histogram of the bootstrap sample means is shown in figure 2.</p>
<p><img src="media/media/image4.jpeg" style="width:4.35251in;height:3in" /></p>
<p>Figure 2. Histogram of the bootstrap sample means of students’ grades.</p>
<p>From figure 2, the bootstrap distribution is close to a normal distribution. With computer technology, we are not limited to just 100 bootstrap samples. We can generate a better histogram of the variability with large amount of bootstrap samples. Figure 3 shows the bootstrap distribution with 10,000 bootstrap sample means.</p>
<p><img src="media/media/image5.jpeg" style="width:4.35251in;height:3in" /></p>
<p>Figure 3. Histogram of bootstrap sample means of 10,000 bootstrap samples of students’ grades.</p>
<p>The general process of creating a bootstrap distribution is</p>
<ul>
<li><p>Generate bootstrap samples by sampling with replacement from the original sample, using the same sample size</p></li>
<li><p>Compute the statistic of interest for each of the bootstrap samples</p></li>
<li><p>Collect the statistics for many bootstrap samples to create a bootstrap distribution</p></li>
</ul>
<p><em>Activity 1:</em> Use the R dataset <strong>mtcars</strong> to perform the following:</p>
<ul>
<li><p>Generate a bootstrap sample of the variable mile per gallon (mpg);</p></li>
<li><p>Repeat the previous process and generate1000 bootstrap samples and calculate the mean of each sample;</p></li>
<li><p>Generate a histogram of the bootstrap sample means.</p></li>
</ul>
<p>What is the distribution of the bootstrap sample mean? What is the approximate mean of the bootstrap sample means? Is that close to the mean of mpg in the original sample?</p>
<p><em>Activity 2:</em> Use the data on <a href="http://www.stat.umn.edu/geyer/3701/data/boot1.csv">www.stat.umn.edu/geyer/3701/data/boot1.csv</a> to do the following</p>
<ul>
<li><p><strong>x</strong> is a data frame, change <strong>x</strong> to a vector</p></li>
<li><p>Generate 1000 bootstrap samples from the original sample of <strong>x</strong> and calculate the mean</p></li>
<li><p>Generate a histogram of the bootstrap sample means</p></li>
</ul>
<p>What is the distribution of the bootstrap sample mean?</p>
<h2 id="theoretical-explanation-of-bootstrap-sampling">Theoretical Explanation of Bootstrap Sampling</h2>
<p>In this section, we will introduce the theoretical approach of bootstrap sampling and bootstrap distribution.</p>
<p>Let <span class="math inline">\(X_{1},X_{2},\ldots X_{n}\)</span> be random variables following the distribution F. The empirical distribution is defined as</p>
<p><span class="math display">\[\widehat{F}\left( x \right) = \frac{1}{n}\sum_{i = 1}^{n}{1\{ X_{i} \leq x\}}\ ,\ \ X \in R\]</span></p>
<p>We are interested in a parameter <span class="math inline">\(\theta = \phi(F)\)</span>. A natural estimate of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\widehat{\theta} = \phi(\widehat{F})\)</span>. A sequence of identical, independently distributed (iid) sample of size n, {<span class="math inline">\(X_{1}^{*},\ X_{2}^{*},\ \ldots,\ X_{n}^{*}\)</span>} are drawn from <span class="math inline">\(\widehat{F}\)</span>, with replacement. Then the sample {<span class="math inline">\(X_{1}^{*},\ X_{2}^{*},\ \ldots,\ X_{n}^{*}\)</span>} is a bootstrap sample. Let <span class="math inline">\({\widehat{\theta}}_{n} = g(X_{1},\ldots X_{n})\)</span> denote some estimator. A general algorithm for the bootstrap distribution is</p>
<ul>
<li><p>Generate bootstrap sample {<span class="math inline">\(X_{1}^{*},\ X_{2}^{*},\ \ldots,\ X_{n}^{*}\)</span>} independently drawn from <span class="math inline">\(\widehat{F}\)</span>, with replacement.</p></li>
<li><p>Compute the estimator <span class="math inline">\({\widehat{\theta}}_{n}^{*} = g(X_{1}^{*},\ \ldots,\ X_{n}^{*})\)</span></p></li>
<li><p>Repeat the previous steps B times and get estimators <span class="math inline">\({\widehat{\theta}}_{n,1}^{*},\ldots,\ {\widehat{\theta}}_{n,B}^{*}\)</span></p></li>
<li><p>Generate the distribution <span class="math inline">\(G_{n,B}\left( t,{\widehat{F}}_{n} \right) = \frac{1}{n}\sum 1\{{\widehat{\theta}}^{*} &lt; t\}\)</span> with the variance <span class="math inline">\(\widehat{s} = \sqrt{\frac{1}{B}\sum_{j = 1}^{B}\left( {\widehat{\theta}}_{n,j}^{*} - \overline{\theta} \right)^{2}}\)</span></p></li>
</ul>
<h2 id="bootstrap-confidence-interval">Bootstrap Confidence Interval</h2>
<p>The variability of bootstrap statistics is similar to the variability of sample statistics if we were to sample repeatedly from the population, so we can use the standard deviation of the bootstrap distribution to estimate the <strong>standard error</strong> of the sample statistic. The standard deviation of the bootstrap statistics in a bootstrap distribution gives a good approximation of the <strong>standard error of the statistic</strong>. We can calculate the standard error of the bootstrap distribution using the R function sd().</p>
<p>Example 3. Use the previous student grade sample in example 1 to generate 100 bootstrap samples, calculate the mean of each bootstrap sample. Calculate the mean of the original sample. Calculate the standard error of the bootstrap sample means. The R code is</p>
<p><img src="media/media/image6.png" style="width:3.67708in;height:1.89583in" /></p>
<p>So the standard error of the bootstrap sample means is 4.15. The mean of the original sample means is 81.</p>
<p>When a bootstrap distribution for a sample statistic is a normal distribution, we can estimate a 95% confidence interval using the following formula</p>
<p><span class="math display">\[(statistic \pm 1.96*SE)\]</span></p>
<p>where SE is the standard error.</p>
<p>So in example 3, a 95% confidence interval for the bootstrap sample mean grade can be</p>
<p><span class="math display">\[\left( 81 \pm 1.96*4.15 \right) = (72.87,89.13)\]</span></p>
<p><em>Activity 3:</em> A sample of prices of skateboards for sale online (Lock et al. 2013) is shown in table 1. Answer the following questions based on the 20 skateboard prices.</p>
<table>
<tbody>
<tr class="odd">
<td>19.95</td>
<td>24.99</td>
<td>39.99</td>
<td>34.99</td>
<td>30.99</td>
<td>92.5</td>
<td>84.99</td>
<td>119.99</td>
<td>19.99</td>
<td>114.99</td>
</tr>
<tr class="even">
<td>44.99</td>
<td>50</td>
<td>84.99</td>
<td>29.91</td>
<td>159.99</td>
<td>61.99</td>
<td>25</td>
<td>27.5</td>
<td>84.99</td>
<td>199</td>
</tr>
</tbody>
</table>
<p>Table 1. Prices of skate boards for sale online</p>
<ul>
<li><p>Generate 50 bootstrap samples of the price and calculate the mean of each sample</p></li>
<li><p>Generate a histogram of the 50 bootstrap sample means</p></li>
<li><p>Is the distribution normal?</p></li>
<li><p>Calculate the mean of the original sample</p></li>
<li><p>Calculate the standard error of the 50 bootstrap sample means</p></li>
<li><p>Construct a 95% confidence interval of the price using the 50 bootstrap samples</p></li>
</ul>
<h2 id="confidence-interval-based-on-bootstrap-percentiles">Confidence Interval based on Bootstrap Percentiles</h2>
<p>Besides the 95% confidence interval, we may be interested in 90% or 99% confidence intervals. If the bootstrap distribution is approximately normal, we can construct a confidence interval by using the percentiles in the bootstrap distribution of statistic to construct a confidence level. This procedure for finding bootstrap confidence intervals are flexible and can be applied in a wide variety of different situations and with many different parameters (Lock, 2013). The process is simple if we already have the bootstrap distribution generated. If we denote the confidence level as <span class="math inline">\(1 - \alpha\)</span>. From the bootstrap distribution, we need to find the end points corresponding to <span class="math inline">\((\frac{\alpha}{2},\ 1 - \frac{\alpha}{2}\)</span>). For example, if we would like to generate a 99% confidence interval, then we need to find the end points under the bootstrap distribution corresponding to the left proportions of <span class="math inline">\(0.005\)</span> and <span class="math inline">\(0.995\)</span> (figure 4)</p>
<p><img src="media/media/image7.png" style="width:100%" /></p>
<p>Figure 4. The 99% confidence interval using the percentile method. The two red triangle points are corresponding to the left proportion of 0.005, and 0.995. So the 99% confidence interval is (21.655, 62.990).</p>
<p>The process of generate a bootstrap confidence interval using the percentile method is:</p>
<ul>
<li><p>Generate the bootstrap distribution of sample statistic</p></li>
<li><p>Use the R function quantile() to get the two end points of a confidence interval corresponding to a confidence level</p></li>
</ul>
<p>Example 4. Use the data <strong>CommuteAtlanta</strong> in R package <strong>Lock5Data</strong>. a) Generate 10,000 bootstrap samples of the commute time (<strong>Time</strong>) and calculate the mean of those 10,000 samples; b) Generate a histogram of the bootstrap sample means; c) Generate the 90% confidence interval using the percentile method.</p>
<p>The R code is</p>
<p><img src="media/media/image8.png" style="width:3.51042in;height:1.76042in" /></p>
<p>The 90% confidence interval is (27.61, 30.68).</p>
<p><em>Activity 4:</em> Use the <strong>CommuteAtlanta</strong> data in the R package <strong>Lock5Data</strong> to do the following:</p>
<ol type="a">
<li><p>Generate 1000 bootstrap samples, and calculate the correlation between distance and time using cor(Distance[], Time[])</p></li>
<li><p>Generate the histogram of the bootstrap sample correlations</p></li>
<li><p>Generate a 96% confidence interval of the correlation</p></li>
</ol>
<h2 id="use-r-package-boot-to-generate-bootstrap-sample-statistics">Use R package boot to generate bootstrap sample statistics</h2>
<p>The R function <strong>boot()</strong> in the package <strong>boot</strong> can generate bootstrap sample statistics and calculate the standard error of the bootstrap distribution of the sample statistic. Firstly, you need to install the package <strong>boot</strong> in R, and require the library of boot by <strong>library(boot)</strong>. The function is</p>
<p><strong>boot( data, statistic, R)</strong></p>
<p>For the arguments:</p>
<p><strong>data</strong>: vector, matrix or data frame. If it is a matrix or data frame then each row is considered as one multivariate observation</p>
<p><strong>statistic</strong>: function which when applied to data returns a vector containing the statistic of interest.</p>
<p><strong>R</strong>: the number of bootstrap samples/replicates.</p>
<p>Example 5: Use the data <strong>Portfolio</strong> in the package <strong>ISLR</strong> to do the following: a) write a function named <strong>“sumvar”</strong> to find var(X+Y); b) run the function for index 1:100; c) use <strong>boot()</strong> to generate 1000 bootstrap samples and use <strong>sumvar</strong> to find the statistic.</p>
<p>What is the standard error of the bootstrap distribution of var(X+Y)?</p>
<p>The R code is</p>
<p><img src="media/media/image9.png" style="width:4.75in;height:3.45833in" /></p>
<p>The original bootstrap sample has the variance of X+Y as 3.69, and the standard error of the bootstrap distribution of var(X+Y) is 0.44.</p>
<p><em>Activity 5:</em> Use the data <strong>mtcars</strong> in R to do the following: a) write a function named <strong>“tstatistic”</strong> to find the t statistic of mpg comparing the mean mpg with 25; b) use <strong>boot()</strong> to generate 1000 bootstrap samples and use <strong>tstatistic</strong> to find the statistic of the samples;</p>
<p>What is the standard error of the bootstrap distribution of the t statistics?</p>
<h3 id="cross-validation-method">Cross-Validation Method</h3>
<p>In multiple regression, we examine methods for selecting the variables to include in a regression equation. When description is our primary goal, the selection and interpretation of a regression model signals the end of our labor. But when our goal is prediction, we can justifiably ask, “How well will this equation perform in the real world?”</p>
<p>By definition, regression techniques obtain model parameters that are optimal for a given set of data. In ordinary least squares regression, the model parameters are selected to minimize the sum of squared errors of prediction (residuals) and, conversely, maximize the amount of variance accounted for in the response variable (R-squared). Because the equation has been optimized for the given set of data, it won’t perform as well with a new set of data.</p>
<p>For example, an exercise physiologist might use regression analysis to develop an equation for predicting the expected number of calories a person will burn while exercising on a treadmill. The response variable is the number of calories burned (calculated from the amount of oxygen consumed), and the predictor variables might include duration of exercise (minutes), percentage of time spent at their target heart rate, average speed (mph), age (years), gender, and body mass index (BMI). If we fit an ordinary least squares regression equation to this data, we’ll obtain model parameters that uniquely maximize the R-squared for this <em>particular</em> set of observations.</p>
<p>But our researcher wants to use this equation to predict the calories burned by individuals in general, not only those in the original study. We know that the equation won’t perform as well with a new sample of observations, but how much will we lose? <em>Cross-validation</em> is a useful method for evaluating the generalizability of a regression equation.</p>
<p>In cross-validation, a portion of the data is selected as the training sample, and a portion is selected as the hold-out sample. A regression equation is developed on the training sample and then applied to the hold-out sample. Because the hold-out sample wasn’t involved in the selection of the model parameters, the performance on this sample is a more accurate estimate of the operating characteristics of the model with new data.</p>
<p>In <em>k-fold cross-validation</em>, the sample is divided into <em>k</em> subsamples. Each of the <em>k</em> subsamples serves as as a hold-out group, and the combined observations from the remaining <span class="math inline">\(k - 1\)</span> subsamples serve as the training group. The performance for the <em>k</em> prediction equations applied to the <em>k</em> hold-out samples is recorded and then averaged. (When <em>k</em> equals <em>n</em>, the total number of observations, this approach is called <em>jackknifing</em>.)</p>
<p>The following listing provides a function (called <strong>shrinkage( )</strong>) for cross-validating a model’s R-square statistic using <em>k</em>-fold cross-validation.</p>
<p><embed src="media/media/image10.png" style="width:5.89666in;height:2.54202in" /></p>
<p>Using the above listing, we define our functions, create a matrix of predictor and predicted values, get the raw R-squared, and get the cross-validated R-squared.</p>
<p>We’ll use the <strong>state.x77</strong> dataset in the base package for an example. Suppose we want to explore the relationship between a state’s murder rate and other characteristics of the state, including population, illiteracy rate, average income, and frost levels (mean number of days below freezing).</p>
<p>Because the <strong>lm( )</strong> function requires a data frame (and the <strong>state.x77</strong> dataset is contained in a matrix), we can simplify our life with the code below. This code creates a data frame called <strong>states</strong>, containing the variables we’re interested in. We’ll use this new data frame for the remainder of the example. (<a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/state.html">https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/state.html</a>)</p>
<p><embed src="media/media/image11.png" style="width:5.31324in;height:0.39589in" /></p>
<p>Now let’s fit the multiple regression model with the <strong>lm( )</strong> function (see figure 5). <embed src="media/media/image12.png" style="width:6.42798in;height:3.77136in" /></p>
<p>Figure 5. Multiple linear regression.</p>
<p>The <strong>shrinkage( )</strong> function is used to perform a 10-fold cross-validation with the <strong>states</strong> data, using the model with all four predictor variables (figure 6).</p>
<p><embed src="media/media/image13.png" style="width:6.27171in;height:0.95847in" /></p>
<p>Figure 6. Use of the <strong>shrinkage( )</strong> function to perform a 10-fold cross-validation with the <strong>states</strong> data.</p>
<p>We can see that the R-square based on the sample (0.567) is overly optimistic. A better estimate of the amount of variance in murder rates that this model will account for with new data is the cross-validated R-square (0.441). (Note that observations are assigned to the <em>k</em> groups randomly, so we’ll get a slightly different result each time we execute the <strong>shrinkage( )</strong> function.)</p>
<p><em>Activity 6:</em> The <strong>boot</strong> package is a library of R code that’s included with the standard installation but isn’t automatically loaded. Load <strong>boot</strong> with a call to <strong>library(boot)</strong>. You’ll find a data frame called <strong>nuclear</strong>, which contains data on the construction of nuclear power plants in the United States in the late 1960s. (<a href="https://vincentarelbundock.github.io/Rdatasets/doc/boot/nuclear.html">https://vincentarelbundock.github.io/Rdatasets/doc/boot/nuclear.html</a>)</p>
<p>Access the documentation by entering <strong>?nuclear</strong> and examine the details of the variables. (Note there is a mistake for <strong>date</strong>, which provides the date that the construction permits were issued – it should read “measured in years since January 1 <strong>1900</strong> to the nearest month.”)</p>
<p>One of the original objectives was to predict the cost of further construction of these power plants. Create a fit and summary of a linear regression model that aims to model <strong>cost</strong> by <strong>t1</strong> and <strong>t2</strong>, two variables that describe the different elapsed times associated with the application for and issue of various permits.</p>
<p>Use the <strong>shrinkage( )</strong> function to perform a 10-fold cross-validation with the <strong>nuclear</strong> data. Use the above model with two predictor variables.</p>
<p>We could use cross-validation in variable selection by choosing a model that demonstrates better generalizability. For example, a model with two predictors (<strong>Population</strong> and <strong>Illiteracy</strong>) shows less R-square shrinkage (0.040 versus 0.126) than the full model (figure 7).</p>
<p><embed src="media/media/image14.png" style="width:5.06321in;height:0.78136in" /></p>
<p>Figure 7. Use of the <strong>shrinkage( )</strong> function to perform a 10-fold cross-validation with the <strong>states</strong> data (two-predictor model).</p>
<p>This may make the two-predictor model a more attractive alternative.</p>
<p>All other things being equal, a regression equation that’s based on a larger training sample and one that’s more representative of the population of interest will cross-validate better. We’ll get less R-squared shrinkage and make more accurate predictions.</p>
<p><em>Activity 7:</em> Returning to the <strong>nuclear</strong> data frame, refit the model. Expand the model that you fit in Activity 6 by including a third predictor variable for the date the construction permit was issued. Use the <strong>shrinkage( )</strong> function to perform a 10-fold cross-validation on this three-predictor model.</p>
<h4 id="cv.glm-for-performing-cross-validation"><strong>cv.glm</strong> for Performing Cross-Validation</h4>
<p>There are a number of packages and functions in R that assist in performing cross-validation. Each has its own limitations or quirks, so rather than going through a number of incomplete functions, we show one that works well for generalized linear models (including linear regression).</p>
<p>The <strong>boot</strong> package by Brian Ripley has <strong>cv.glm</strong> for performing cross-validation. As the name implies, it works only for generalized linear models, which will suffice for generalized linear models, which will suffice for a number of situations. (<a href="https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm">https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm</a>)</p>
<p><embed src="media/media/image15.png" style="width:5.80289in;height:2.18781in" /></p>
<p>The results from <strong>cv.glm</strong> include <strong>delta</strong>, which has two numbers, the raw cross-validation error based on the cost function (in this case the mean squared error, which is a measure of correctness for an estimator) for all the folds and the adjusted cross-validation error. This second number compensates for not using leave-one-out cross-validation, which is like <em>k</em>-fold cross-validation except that each fold is the all but one data point with one point held out.</p>
<p>While we got a nice number for the error, it helps us only if we can compare it to other models, so we run the same process for the other model we built, rebuilding it with <strong>glm</strong> first.</p>
<p><embed src="media/media/image16.png" style="width:6.28213in;height:2.97958in" /></p>
<p>Once again, the second model, <strong>fit2_glm</strong>, is the superior model. Figure 8 shows how much AIC and cross-validation agree on the relative merits of the different models. The scales are all different but the shapes of the plots are identical.</p>
<p><img src="media/media/image17.png" style="width:6.15548in;height:3.75995in" /></p>
<p>Figure 8. Plots for cross-validation error (raw and adjusted) and AIC for <strong>states</strong> models. The scales are different, as they should be, but the shapes are identical, indicating that <strong>fit2_glm</strong> is the best model.</p>
<p>Following is the R code used to generate the plots in figure 8:</p>
<p><embed src="media/media/image18.png" style="width:6.15711in;height:2.59411in" /></p>
<p>Determining the quality of a model is an important step in the model building process. This can take the form of traditional tests of fit or more modern techniques like cross-validation. These can all be shaped by helping select which variables are included in a model and which are excluded.</p>
<p><em>Activity 8:</em> Use the <strong>cv.glm( )</strong> function to compare the two models in activities 6 and 7.</p>
<p><em>Activity 9:</em> Generate plots for cross-validation error (raw and adjusted) and AIC for the two models that you created for the <strong>nuclear</strong> data.</p>
<h2 id="references">References</h2>
<p>Baumer, B.S., Kaplan, D.T., and Horton, N.J., 2017. Modern Data Science with R, CRC Press.</p>
<p>James, G., Witten, D., Hastie, T., and Tibshirani R., 2014. An Introduction to Statistical Learning with Applications in R. 2<sup>nd</sup> edition, Springer</p>
<p>Lock, R. H., Lock, P. F., Morgan K.L., Lock E.F., and Lock D.F., 2013. Statistics: Unlock the Power of Data, Wiley.</p>
<p>Mackisack, M., “Favourite Experiments: An Addendum to What Is the Use of Experiments Conducted by Statistics Students?’ <em>Journal of Statistics Education</em>, 1994, <a href="http://www.amstat.org/publications/jse/v2n1/mackisack.supp.html">http://www.amstat.org/publications/jse/v2n1/mackisack.supp.html</a></p>
</body>
</html>
