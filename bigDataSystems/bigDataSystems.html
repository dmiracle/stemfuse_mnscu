<p><strong>Module: Big Data Systems</strong> <strong>– 1 Credit / 15 hours</strong></p>
<p><strong>Competency: Big Data</strong></p>
<p>Authors: Jack Pope, Prabesh Shrestha</p>
<p><strong>Instructor overview &amp; objective</strong></p>
<p>This course module is about distributed computing environments for “big data.” It looks at systems that can perform parallel computation or cluster computing. Topics include shared memory, map-reduce, clustering, concurrency, task parallel versus data parallel, database integration, multi-threading</p>
<p>and networked file systems.</p>
<p>Given that this module is to be worth 1 credit, or 15 hours of coursework, the instructor should prepare three to five subtopic discussions. These should account for at least 5 hours of instructional material and might be in the form of lecture notes or lecture videos. The additional hours should be comprised of student labs or assignments.</p>
<p>Our examples use Apache Spark and Apache Hadoop. Spark makes for distributed in-memory computing and can write to local as well as networked file systems. A distributed file system called the Hadoop Distributed File System (HDFS) is typically installed with Apache Spark in the enterprise.</p>
<p>Spark essentially allows us to process large data sets faster by employing more than one machine. Spark also easily integrates with popular programming languages, databases and other tools for ensuring high availability.</p>
<p>The lessons in this module use the Python and SQL programming languages on distributed instances of Apache Spark and Apache Hadoop. Installation and configuration of virtual machines takes place on Oracle Virtual Box and Cloudera.</p>
<p>While our example system topology has 8GB of RAM and 32GB disk space per compute node, students can specify fewer resources for testing small data sets. Depending on the operating system for installing Spark, you may be fine with a single 4GB master node for testing and training. Even fewer local resources are required if the set up is in the cloud. Cloud providers who accommodate Spark and Hadoop include Cloudera, Databricks, AWS, Microsoft Azure, Google Cloud, Oracle Cloud and IBM Bluemix.</p>
<p><strong>Schedule</strong></p>
<p>Unit Topic</p>
<p>========================================</p>
<p>1 Overview</p>
<p>Paradigms for Big Data processing</p>
<p>The map-reduce paradigm</p>
<p>2 Considerations for concurrency and parallelism</p>
<p>Computational scales</p>
<p>Characteristics of Big Data</p>
<p>The origin of “big data”</p>
<p>Types of data</p>
<p>Exercise</p>
<p>3 Apache Spark overview</p>
<p>Installation and configuration of Apache Spark</p>
<p>Testing the installation of Apache Spark</p>
<p>Running a Spark cluster</p>
<p>Submitting jobs to the Spark cluster</p>
<p>Exercise</p>
<p>4 Apache Hadoop overview<br />
Application of MapReduce in Hadoop</p>
<p>Installation and configuration of Hadoop on the Cloudera Virtual Machine</p>
<p>Testing the installation of Apache Hadoop</p>
<p>Exercise</p>
<p><strong>Overview</strong></p>
<p>The problem, or What is Big Data?</p>
<p>One definition of Big Data, according to Jack Pope: <em>when the value of your data needs exceeds the value of your local processing resources.</em></p>
<p>By this definition, Big Data processing cannot take place on a single machine. Instead, many machines, a “cluster,” must be used in tandem. These machines can be local or virtual machines in the cloud.</p>
<p>For our big data analytics in this course module we will emphasize Apache Spark. Spark makes for distributed in-memory computing and can write to local as well as networked file systems. A distributed file system called the Hadoop Distributed File System (HDFS) is typically installed with Apache Spark in the enterprise.</p>
<p>Spark essentially allows us to process large data sets faster by employing more than one machine. There are other systems that do as much. However, Spark is also somewhat easily integrates with popular programming languages, databases and other tools for ensuring high availability.</p>
<p>A few points of information need to be made to distinguish Big Data Systems from Big Data Analytics. Big Data Systems are platforms. Before installing and configuring a Big Data System, the analytical problem should be understood. This is because a lot of effort and time can go into building a particular Big Data System that is not suitable for the problem, and sometimes we just don't need a sledgehammer to drive a tack.</p>
<p><strong>The map-reduce paradigm</strong></p>
<p>When faced with huge data sets we are motivated to avoid serial in design and execution, as it can tax patience. Instead, we try to break a workload into parts that can be processed independently using multiple machines. Such parallelization may or may not be feasible given the problem at hand, and we have to think about what problems can be solved in asynchronous task parallel and data parallel fashion.</p>
<p>Processing of partitioned data on cluster nodes takes place independently and concurrently. Results can be consolidated on a master node according to the keys of key-value pairings in the data.</p>
<p>While the map reduce concept goes back a to the time when Lisp was at is zenith, Google managed to win a patent for its version, spelled MapReduce (USPTO patent #7,650,331).</p>
<p>The map-reduce paradigm supports parallel computing. The map function as applied to each value of a data set is readily performed in parallel, as the the value and the function are independent. However, in the context of distributed processing, a reduce operation will depend on a completed map operation.</p>
<p>map and reduce definitions:</p>
<p>Fundamentally, the map and reduce functions each take as parameters a data set and a function, which is then applied to each element of data. The map function then returns a transformed data set. The reduce function returns an aggregate of the the data set.</p>
<ul>
<li><p>map definition 2: index and partition input data into a subset for each compute node. In tree structured clusters, worker nodes are masters of subordinate nodes.</p></li>
<li><p>reduce definition 2: the master node consolidates compute node output into one data set. Reduction may entail sorting, averaging, filtering, etc.</p></li>
</ul>
<p>In Apache Spark, map and reduce work on local data, and the map-reduce paradigm is injected with “shuffle” and “sort” operations. We have the following definitions:</p>
<ul>
<li><p>map definition 3: Worker nodes apply the map function to their respective local data, maintaining results in temporary storage or memory. Map operations occur on each partition element and include transforming or filtering.</p></li>
<li><p>shuffle: The transfer of values from the mapper to the reducer. Worker nodes are assigned a partition of data associated with a sorted range of keys. The keys are generated by a dedicated key-value map function.</p></li>
<li><p>sort: a dependency for shuffle, with ordering by keys, then value.</p></li>
<li><p>reduce definition 3: Worker nodes aggregate (sum, count) all partition values by key.</p></li>
</ul>
<p>The pseudo code for a map:</p>
<p>map(key, value) → list(key, value')</p>
<p>The pseudo code for a map:</p>
<p>reduce(key, list(value')) → (key, value'')</p>
<p>Apache Hadoop's implementation of Google's MapReduce entails additional operations such as split and combine. The following illustration provides some clarity (Source: <a href="https://www.mssqltips.com/sqlservertip/3222/big-data-basics--part-5--introduction-to-mapreduce/"><span class="underline">https://www.mssqltips.com/sqlservertip/3222/big-data-basics--part-5--introduction-to-mapreduce/</span></a> )</p>
<p><img src="./media/media/image1.png" style="width:4.1125in;height:2.99653in" /></p>
<p>In the past decade, the general map-reduce concept has been convoluted by various implementations, including complex ones like that of MapReduce for Hadoop. Apache Spark does not require Hadoop Google's MapReduce. Spark employs the map-reduce paradigm in partitioning RDDs and applying transformation to data in dedicated areas of distributed memory.</p>
<p><strong>Considerations for concurrency and parallelism</strong></p>
<p>Parallel operations can run at different levels of granularity:</p>
<p>- bit-level – processing multiple bits of data in parallel</p>
<p>- instruction-level – executing different instructions from the same instruction stream in parallel</p>
<p>- task-level – executing separate instruction streams in parallel</p>
<p>Different forms of parallel hardware:</p>
<p>- multi-core processors</p>
<p>- symmetric multiprocessors (multiple CPUs on board)</p>
<p>- general purpose graphics processing unit</p>
<p>- field-programmable gate arrays</p>
<p>- computer clusters</p>
<p>Embarrassingly parallel: very little or no effort is required to separate problem into parallel tasks.</p>
<p>Task parallel versus data parallel … simultaneous execution on multiple cores of:</p>
<p>- Task parallel: different functions, same data.</p>
<p>- Data parallel: same function, different data (a partitioned data set; the parallel for-loop is data parallel).</p>
<p>Some tasks cannot be parallelized, as when the work on each core depends on the results of another core.</p>
<p>Data parallel scheduler: efficiently balance the workload across cores. Only those workloads that are likely to be time consuming should be parallelized. Certain thresholds should be considered.</p>
<p>Distributed systems:</p>
<p>-Mutli-core CPUs</p>
<p>-Message passing Interface (MPI)</p>
<p>-Parallel Virtual Machine (PMI)</p>
<p>-HTCondor</p>
<p>-Mosix and OpenMosix</p>
<p>-Boinc</p>
<p>A Big Data System wish list:</p>
<p>Our platform should run as a service for big compute jobs, either locally, in the cloud or as a hybrid. Ideally, the system should facilitate selecting on-line data targets, uploading files, selecting various statistical options, and then running a program.</p>
<p>The system should scale from local servers to remote private and public cloud nodes. By allowing private clouds to join in parallel computing efforts, the system facilitates peer-to-peer like clouds.</p>
<p>There should be a status monitor program that communicates node status in terms of availability, job progress, and errors.</p>
<p>The system should alert the user to problems such as corrupt files and failed data error checks. Otherwise a node might finish a job with erroneous output, or it may never finish if bad data or mistaken code does not ever satisfy some while loop, etc. Without timely notification of such a problem, the cost of leased resources could escalate without any benefit in return.</p>
<p>Regardless of the promoted fault tolerances of system, such as Apache Spark, we cannot take this for granted.</p>
<p>Our examples use Apache Spark and Apache Hadoop. Spark makes for distributed in-memory computing and can write to local as well as networked file systems. A distributed file system called the Hadoop Distributed File System (HDFS) is typically installed with Apache Spark in the enterprise.</p>
<p>Spark essentially allows us to process large data sets faster by employing more than one machine. Spark also easily integrates with popular programming languages, databases and other tools for ensuring high availability.</p>
<p><strong>Computational scales</strong></p>
<p>Single computing node:</p>
<p>In this type of computing, a single processor is used to compute the data stored in internal or external hard drives. The storage and processing capability of the single node may not be sufficient for big data analysis.</p>
<p>Parallel computing:</p>
<p>Parallel computing refers to a very large number of single computing nodes; these nodes usually have specialized capabilities and are connected with each other via a network. Specialized computers are comparatively expensive.</p>
<p>Commodity Clustering:</p>
<p>Commodity clusters are affordable parallel computers with comparatively lower number of computer nodes. These computers have generic computing capabilities and are not as powerful as traditional parallel computers hence they are comparatively cheaper and is commonly used for distributed computing.</p>
<p>In commodity clustering, computers/nodes are arranged in a rack, and connected to each other via a fast network. Computing in one or more of such clusters over a network is called Distributed Computing. Such architectures enable data parallelism.</p>
<p><strong>Characteristics of Big Data</strong></p>
<p>The characterization of big data using the 3 V’s was introduced by Doug Laney of Gartner. According to Laney [6]:</p>
<ul>
<li><p>Volume: This refers to the vast amounts of data that is generated continuously through machines, people or organization.</p></li>
<li><p>Velocity: This refers to the speed at which data is being generated and the pace at which data moves from one point to the next.</p></li>
<li><p>Variety: Data generated by different source could be structured, numeric data in traditional databases to unstructured text documents, email, video, audio, stock ticker data, geospatial and financial transactions.</p></li>
</ul>
<p>In addition to these 3, new V’s has been added to the definition:</p>
<ul>
<li><p>Veracity: Refers to bias, noise, and uncertainty in the data.</p></li>
<li><p>Variability: In addition to the increasing velocities and varieties of data, data flows can be highly inconsistent with periodic peaks. Daily, seasonal and event-triggered peak data loads can be challenging to manage, specially the unstructured data [1].</p></li>
</ul>
<p>The above V’s are the dimensions that characterize big data, and embody its challenges: huge amounts of data, in different formats and varying quality that must be processed quickly.</p>
<p>Businesses processes big data to understand hidden patterns, trends or anomalies for any problem. This will help in understanding the problems within the organization and help the business make more informed and data driven decisions. This leads us to next V – value.</p>
<p>Value: Processing big data must bring about value from the insights gained [6].</p>
<p><strong>The origin of “big data”</strong></p>
<p>Infrastructure/Smart devices/Machines:</p>
<p>The flight status app gets its data from the airline sensors, ADS-B and other sensors in and around airport. These help track flights.</p>
<p>Health sensor data like Fitbit are another example of big data. They can track various activities of any user and provide data continuously forming large chunk of data for any data analysis.</p>
<p>In addition to that, different smart devices and sensors can interact with each other, for example, the traffic light changes based on its interaction with the sensors that detects the traffic on the road. This function generates data and the process is termed as Internet of Things (IoT).</p>
<p>These data form the largest source of big data.</p>
<p>People:</p>
<p>Another source of big data is people. The data are generated using social media. The other source of data generated by people are online photo sharing, video sharing, blogs, and comments on different websites or documents.</p>
<p>Most of the data from people are text heavy and unstructured hence cannot be conformed in a well-defined data model.</p>
<p>Organization:</p>
<p>Every business have a specific process that they follow to complete their job functions. These processes generates data and these data could be retail transactions, credit card transactions, medical records, ecommerce or stocks. Traditionally, these data are structured and can be stored in a table in relational database management system.</p>
<p><strong>Types of data</strong></p>
<p>Structured:</p>
<p>This refers to traditional data that are well organized and stored in a relational model or table. Since this type of data is organized, searching and accessing information is very easy.</p>
<p>Semi structured:</p>
<p>This refers to data that do not conform to the structure as defined by structured data but contains markers to separate semantic elements and enforce hierarchies of records and fields within the data. Web data such JSON (JavaScript Object Notation) files, csv files, tab-delimited text files, XML are examples of Semi-structured data</p>
<p>Unstructured:</p>
<p>This refers to data that cannot conform to well-defined data model. These include text, images, audio or video. They cannot be stored in relational model but are the major types of data collected and stored for data analysis</p>
<p><strong>Exercise</strong></p>
<p><strong>Using the link:</strong> <strong><a href="https://flightaware.com/about/faq#intl"><span class="underline">https://flightaware.com/about/faq#intl</span></a> and <a href="https://www.faa.gov/air_traffic/by_the_numbers/"><span class="underline">https://www.faa.gov/air_traffic/by_the_numbers/</span></a> , have student gather information on how flight status app works, and complete the following task:</strong></p>
<ul>
<li><p><strong>Find the different sources and types of data used by the app</strong></p></li>
<li><p><strong>Is the data obtained by flight app – a big data? If yes, explain using the Vs</strong></p></li>
<li><p><strong>How does parallelism and concurrency help the users’ access flight status?</strong></p></li>
</ul>
<p><strong>Distributed file systems (DFS)</strong></p>
<p>Data are generated by different sources and if a business wants to store the data, they need to store it as a file. When a file is created, it is stored in a memory in specific order or sequence determined by the operating system of the computer and is called file system.</p>
<p>If the amount of data is small, the data can be stored in personal computer and managed individually, however, if the volume of data is high, multiple computers might be required to store the data. If we have large volume of data that do not fit in specific hard drive and are distributed across multiple computer and managed for easy data access, it is called Distributed File System (DFS). DFS contains group of computer nodes within a local area network or wide area network.</p>
<p>When a large volume of data needs to be stored using DFS, parts of data is replicated across the nodes of the DFS, between the nodes and computers. When data is required for analysis, computation is moved to the data and not the other way round.</p>
<p>Since the data is replicated within each cluster, failure or one node or rack doesn’t fail the data accessibility hence having data using DFS increases fault tolerance. Also, if the data is popular, having replicated data accessible in parallel, for multiple users, increases concurrency.</p>
<p>DFS also helps with scalability as nodes could be added without affecting other terminals or processes.</p>
<p><span class="underline">Example to understand DFS:</span></p>
<p>Let us consider that we have 10 machines with 1 TB as their memory/storage/hard drives. If we install Hadoop on these machines, we can treat the cluster as one computer with 10 TB data of hard drive capacity in one machine.</p>
<p>Since the data is distributed in multiple machines, we can perform distributed and parallel computation. If it takes 30 minutes to process a 1 TB data, simple mathematics state that it will take 30 X 10 or 300 minutes to process a 10 TB of data. However, with 10 different computers working parallel, we can complete the task in 30 minutes instead of 300 minutes.</p>
<p>In addition to that, if 10 TB is not enough for storage, we can increase or add another computer with higher hard disk without affecting other terminals. This helps with scalability.</p>
<p><strong>Apache Spark</strong></p>
<p>To analyze “Big Data” one system we use is Apache Spark.</p>
<p>Spark essentially allows us to process large data sets faster by employing more than one machine. Spark also easily integrates with popular programming languages, databases and other tools for ensuring high availability.</p>
<p>On Spark, we are using PySpark (Python) for programming applications for a Spark cluster. PySpark is included in the installation of Apache Spark 2.3.1 and it works with GraphX, MlLib and Spark SQL libraries that come with Apache Spark. You must separately install Python (version 2). PySpark is part of the Spark Core library in Apache Spark.</p>
<p>Spark libraries include:</p>
<ul>
<li><p>Spark Core – provides task distributions, scheduling, basic I/O, and an API for Java, Python, Scala and R.</p></li>
<li><p>Spark SQL – relies on JDBC and treats tables as RDDs so that queries are Spark operations.</p></li>
<li><p>MLlib (deprecated to ML) – machine learning algorithms.</p></li>
<li><p>GraphX – graph algorithms.</p></li>
<li><p>Spark Streaming – real-time stream data processing.</p></li>
</ul>
<p><strong>Installation and configuration of a 3 node cluster (using VirtualBox) </strong></p>
<p>If you have VirtualBox, you can set up a cluster using a preferred OS. The nodes for our example will use Ubuntu Linux. These can be cloned for larger clusters locally or migrated to the cloud for hybrid local-cloud cluster.</p>
<p>Dependencies:</p>
<ul>
<li><p>Software: Python, Java, Scala, Spark, SSH (passwordless)</p></li>
<li><p>Network: Routable nodes; same subnet; resolvable hostnames</p></li>
</ul>
<p>Spark configuration:</p>
<ul>
<li><p>spark-defaults.conf</p></li>
<li><p>spark-env.sh</p></li>
<li><p>High availability (HA – multi-nodes): Can optionally use Apache Zookeper</p></li>
</ul>
<p>Running:</p>
<ul>
<li><p>start-master.sh</p></li>
<li><p>start-slave.sh</p></li>
</ul>
<p>Monitoring: Web UI</p>
<p>Example system topology:</p>
<p>host name memory (gb) disk space (gb) ipAddress</p>
<p>SparkMaster 8 32 192.168.1.101</p>
<p>SparkWorker1 192.168.1.102</p>
<p>SparkWorker2 192.168.1.103</p>
<p>For disks, use the full disk space and LVM (we can resize later as needed)</p>
<p>On VirtualBox, the first host to create we'll call SparkMaster. From VirtualBox, we are using a Ubuntu 18 ISO to fulfill the system; with “bridged” networking mode.</p>
<p>SparkMaster components to install:</p>
<ul>
<li><p>Java (Open JDK 8; later versions currently conflict with sbt below, and earlier versions do not support Java lambdas for functional programming.)</p></li>
<li><p>Git</p></li>
<li><p>Scala (includes scalac compiler - Need version 2.12 to avoid compile errors.. see install below)</p></li>
<li><p>sbt – Scala Build Tool</p></li>
<li><p>Python 2</p></li>
<li><p>SSH server</p></li>
</ul>
<p>Latest stable version of Spark from <a href="http://spark.apache.org/downloads.html"><span class="underline">http://spark.apache.org/downloads.html</span></a></p>
<p>(Build here from source, without Hadoop)</p>
<p>Install procedure (all on SparkMaster):</p>
<p>sudo echo &quot;deb https://dl.bintray.com/sbt/debian /&quot; | sudo tee -a /etc/apt/sources.list.d/sbt.list</p>
<p>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 –recv \</p>
<p>2EE0EA64E40A89B84B2DF73499E82A75642AC823</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install openjdk-8-jdk python openssh-server</p>
<p>Scala and sbt installs:</p>
<p>sudo apt remove scala-library scala # incase old version is installed</p>
<p>wget www.scala-lang.org/files/archive/scala-2.12.6.deb</p>
<p>sudo dpkg -i scala-2.12.6.deb</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install scala</p>
<p>sudo apt-get install sbt</p>
<p>Spark installs:</p>
<p>wget <a href="https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1.tgz"><span class="underline">https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1.tgz</span></a></p>
<p>tar xvf spark-2.3.1.tgz</p>
<p>sudo mv spark-version-bin-hadoop2.6 /opt/spark</p>
<p>cd /opt/spark</p>
<p>git init</p>
<p>sudo sbt assembly</p>
<p>./build/sbt assembly</p>
<p>This build seems to take some patience. Go have some coffee.</p>
<p>Now make an environment variable for a Spark user:</p>
<p>echo -e &quot;\nexport SPARK_HOME=/opt/spark&quot; &gt;&gt; ~/.bashrc</p>
<p>echo -e &quot;\nexport PATH=${SPARK_HOME}/bin:${PATH}&quot; &gt;&gt; ~/.bashrc</p>
<p>source ~/.bashrc</p>
<p>source ~/.profile</p>
<p><strong>Testing the installation – running Spark</strong></p>
<p>sudo ./sbin/start-master.sh -h localhost -p 7077 --webui-port 8080</p>
<p>If when attempting the above command, there is this error message:</p>
<p>“Failed to find Spark jars directory (/Users/bh/downloads/spark/assembly/target/scala-</p>
<p>2.10/jars).You need to build Spark with the target “package” before running this program.”</p>
<p>… then, within /opt/spark:</p>
<p>./build/sbt assembly</p>
<p>build/sbt package</p>
<p>If you get “permission denied” errors, then you either have to run the the commands as root user, or using sudo, change the ownership of all file in /opt/spark to your spark user. The latter is probably best, since a user with higher privilege can still use the files.</p>
<p>sudo chown -R SparkUser:SparkUser $SPARK_HOME</p>
<p>The web interface should now be visible at: <a href="http://IPaddress_SparkMaster:8080/"><span class="underline">http://IPaddress_SparkMaster:8080</span></a></p>
<p>Test the Spark and PySpark terminals:</p>
<p>pyspark</p>
<p>&gt;&gt;&gt; quit()</p>
<p>spark-shell</p>
<p>scala&gt; :quit</p>
<p>The web interface is visible at the default port 4040: <a href="http://IPaddress_SparkMaster:8080/"><span class="underline">http://IPaddress_SparkMaster:4040</span></a></p>
<p>At this point we can run programs on the single master node with Scala and Python. Let us run a test now.</p>
<p>$SPARK_HOME/bin/run-example SparkPi 10</p>
<p>$SPARK_HOME/bin/spark-submit examples/src/main/python/pi.py 10</p>
<p>More tests at <a href="https://spark.apache.org/docs/latest/"><span class="underline">https://spark.apache.org/docs/latest/</span></a></p>
<p>Create cluster (workers):</p>
<p>Clone SparkMaster as SparkWorker</p>
<p>- change hostname from SparkMaster to SparkWorker1, 2, …</p>
<p>- change static ip address</p>
<p>- public SSH key (of master put on workers)</p>
<p>- Edit /etc/hosts on each system</p>
<p>Creating a static IP on Ubuntu 18 requires we edit the file /etc/netplan/01-netcfg.yaml to include the following information (including relative indentation):</p>
<p>network:</p>
<p>version: 2</p>
<p>renderer: networkd</p>
<p>ethernets:</p>
<p>enp0s3:</p>
<p>dhcp4: no</p>
<p>addresses: [192.168.1.101/24]</p>
<p>gateway4: 192.168.1.1</p>
<p>nameservers:</p>
<p>addresses: [192.168.1.1]</p>
<p>For each node to communicate by host name, rather than by ip address, add to your /etc/hosts file information for resolving the respective IP addresses (your subnet may of course differ):</p>
<p>127.0.0.1 localhost</p>
<p>192.168.1.101 SparkMaster</p>
<p>192.168.1.102 SparkWorker1</p>
<p>192.168.1.103 SparkWorker2</p>
<p>Generate the SSH key:</p>
<p>cd ~/.ssh</p>
<p>ssh-keygen … for each question, just enter, enter, enter</p>
<p>cp id_rsa.pub authorized_keys</p>
<p>This last step at least allows the workers to have this key once they are cloned from SparkMaster.</p>
<p>As root user, edit /etc/ssh/ssh_config to uncomment the following:</p>
<p>StrictHostKeyChecking no</p>
<p>… this prevents yes/no question when connecting.</p>
<p>Preparation for File IO:</p>
<p>When it comes to using files on multiple nodes you have to ensure the data is distributed. Your options include implementing a distributed file systems on-the-fly by scripting SSHFS, using a networked file system or Hadoop (HDFS). Given a small static cluster, it may be sensible to script a quick file transfer/cleanup that you call from your PySpark program.</p>
<p><strong>Integrating an SQL oriented database</strong></p>
<p>Spark SQL can connect to a database via the OBDC library. Keep in mind that if all compute nodes connect to a single database server, that server may become a bottleneck. Alternatively, you can use serverless SQLite (all nodes will need their own installation of SQLite).</p>
<p>More: <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases"><span class="underline">http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases</span></a></p>
<p>For an SQLite install, for get SQLite:</p>
<p>wget <a href="https://bitbucket.org/xerial/sqlite-jdbc/downloads/sqlite-jdbc-3.8.11.2.jar"><span class="underline">https://bitbucket.org/xerial/sqlite-jdbc/downloads/sqlite-jdbc-3.8.11.2.jar</span></a></p>
<p>Move the jar to the appropriate directory:</p>
<p>/usr/share/java/sqlite-jdbc-3.23.1.jar</p>
<p>Get some test data:</p>
<p>wget <a href="https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite"><span class="underline">https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite</span></a></p>
<p>Start PySpark as:</p>
<p>pyspark --driver-class-path .:/usr/share/java/sqlite-jdbc-3.23.1.jar</p>
<p>In PySpark session, enter:</p>
<p>df = sqlContext.read.format('jdbc'). \</p>
<p>options(url='jdbc:sqlite:/home/.../data/Chinook_Sqlite.sq</p>
<p>dbtable='employee',driver='org.sqlite.JDBC').load()</p>
<p>Test:</p>
<p>df.printSchema()</p>
<p>Class paths can also be added to $SPARK_HOME/conf/spark-defaults.conf as:</p>
<p>spark.driver.extraClassPath /path/to/my.jar</p>
<p><strong>Cloning SparkWorker from SparkMaster</strong></p>
<p>Shutdown SparkMaster to clone it.</p>
<p>$SPARK_HOME/sbin/stop-master.sh</p>
<p>sudo shutdown now</p>
<p>In VirtualBox, right-click on SparkMaster and select “Linked Clone.” Then specify worker node names SparkWorker1, SparkWorker2, … (If you soon plan to migrate the VMs to remote hardware, you can alternatively select “Clone” instead of Linked Clone. Otherwise, go with linked clones now and convert them to full clones later.)</p>
<p>Virtual clone definition (according to VMWare):</p>
<p>Full-clone: A full clone is an independent copy of a virtual machine that shares nothing with the parent virtual machine after the cloning operation. Ongoing operation of a full clone is entirely separate from the parent virtual machine</p>
<p>Linked-clone: A linked clone is a copy of a virtual machine that shares virtual disks with the parent virtual machine in an ongoing manner. This conserves disk space, and allows multiple virtual machines to use the same software installation</p>
<p>For each clone, reconfigure as follows:</p>
<p>- edit /etc/hostname and replace SparkMaster with SparkWorker1</p>
<p>- edit /etc/netplan/01-netcfg.yaml so that the static ip address of SparkWorker is used. So, for SparkWorker1 it is 192.168.1.102</p>
<p>Define a cluster group in VirtualBox (optional):</p>
<p>VirtualBox allows us to define groups, as in cluster group. To do so, right-click on a machine, click add-group, then rename the group. Then using the mouse, drag the other virtual machines into the group. VMs can also be part of more than one group. (All this can all be done on the command-line as well.)</p>
<p>Defining a cluster group in VirtualBox allows for powering the entire cluster from one click. You can start/stop grouped VMs in tandem, from any state (including headless mode … hold the shift-key, click the group and click start).</p>
<p><strong>Testing the local installation of Spark</strong></p>
<p><a href="https://spark.apache.org/docs/latest/spark-standalone.html"><span class="underline">https://spark.apache.org/docs/latest/spark-standalone.html</span></a></p>
<p>Start up the VirtualBox SparkCluster group in headless mode, then log into the master and enter the command:</p>
<p>$SPARK_HOME/sbin/start-master.sh -h 127.0.0.1 -p 7077 --webui-port 8080</p>
<p>This will start a master node with local ip 127.0.0.1 and port 7077. The web interface is visible from other machines via <a href="http://IPaddress_SparkMaster:8080/"><span class="underline">http://IPaddress_SparkMaster:8080</span></a></p>
<p>On each worker enter:</p>
<p>$SPARK_HOME/sbin/start-slave.sh spark://IPaddress_SparkMaster:7077</p>
<p><strong>Running a Spark cluster</strong></p>
<p>The above run commands contrast with the automatic launch of workers from the master, provided they are listed in $SPARK_HOME/conf/slaves and the master's public ssh key resides on each worker.</p>
<p>Make the following edits in $SPARK_HOME/conf</p>
<p>cp spark-defaults.conf.template spark-defaults.conf</p>
<p>echo -e &quot;spark.master \t spark://SparkMaster:7077&quot; &gt;&gt; spark-defaults.conf</p>
<p>cp spark-env.sh.template spark-env.sh</p>
<p>echo SPARK_MASTER_IP=SparkMaster &gt;&gt; spark-env.sh</p>
<p>Edit $SPARK_HOME/conf/slaves</p>
<p>cp slaves.template slaves</p>
<p>echo SparkWorker1 &gt;&gt; slaves</p>
<p>echo SparkWorker2 &gt;&gt; slaves</p>
<p>On SparkMaster, the whole cluster can be expediently launched as:</p>
<p>$SPARK_HOME/sbin/start-all.sh</p>
<p>The above command should be reaching out to the worker node to run the following commands:</p>
<p>$SPARK_HOME/sbin/start-master.sh</p>
<p>$SPARK_HOME/sbin/start-slave.sh spark://SparkMaster:7077</p>
<p>Check the SparkMaster web UI at http://SparkMaster:8080</p>
<p>Check the SparkWorker web UI at http://SparkWorker:8081</p>
<p><strong>Submitting jobs to the Spark cluster</strong></p>
<p>We are running jobs on the cluster without so called High Availability (HA). That means that SparkMaster is a single point of failure (SPOF) for the cluster; if SparkMaster fails then no other node can take over and the job fails. HA can be integrated into Spark by setting up a standby master. The master and standby master are toggled according to rules set by distributed coordination software (like a load balancer), such as Apache Zookeeper.</p>
<p>Programs that run on the cluster will utilize the Spark API (covered in Big Data Analytics module).</p>
<p>Example programs that you can try are located in $SPARK_HOME/lib/spark-examples.</p>
<p>Test using example program (in terminal of any node):</p>
<p>RESULT=`$SPARK_HOME/bin/run-example SparkPi 10`</p>
<p>echo $RESULT → Pi is roughly 3.142803142803143</p>
<p>While the program is being run you will see dozens of output lines regarding systems status. However, the result is of interest. Hence the assignment to the result variable above.</p>
<p>More tests:</p>
<p>RESULT=`$SPARK_HOME/bin/spark-submit \</p>
<p>$SPARK_HOME/examples/src/main/python/pi.py 10`</p>
<p>echo $RESULT → Pi is roughly 3.140904</p>
<p>RESULT=`$SPARK_HOME/bin/spark-submit \</p>
<p>$SPARK_HOME/examples/src/main/scala/org/apache/spark/examples \</p>
<p>SparkPi.scala 10`</p>
<p>echo $RESULT → Pi is roughly 3.140904</p>
<p>$ spark-submit --class org.apache.spark.examples.SparkPi \</p>
<p>--master spark://SparkMaster:7077 --driver-memory 512m \</p>
<p>--executor-memory 512m --executor-cores 1 \</p>
<p>$SPARK_HOME/lib/spark-examples*.jar 10</p>
<p>Stop everything:</p>
<p>$ $SPARK_HOME/sbin/stop-all.sh</p>
<p>Check the status of a job (Here is status of Pope's three nodes) at <a href="http://SparkMaster:8080/"><span class="underline">http://SparkMaster:8080</span></a></p>
<p><img src="./media/media/image2.png" style="width:6.16597in;height:4.46181in" /></p>
<p><strong>Hadoop Overview</strong></p>
<p>The Hadoop platform consists of different services available for analyzing big data. It has 3 main components:</p>
<ul>
<li><p>HDFS: Hadoop Distributed File System</p></li>
<li><p>YARN: Schedule Resource Manager</p></li>
<li><p>MapReduce: Programming model for processing big data</p></li>
</ul>
<p>In addition to MapReduce, there are many programs available for use in Hadoop. Some programming models are independent of Hadoop. Most use HDFS and are managed using YARN. These programs are as follows and can be seen in the architectural diagram too [3]:</p>
<ul>
<li><p>Spark (In-memory Data Processing)</p></li>
<li><p>PIG, HIVE (Data Processing Services using SQL like Query)</p></li>
<li><p>HBase, Cassandra (NoSQL Database)</p></li>
<li><p>Mahout, Spark MLlib (Machine Learning)</p></li>
<li><p>Apache Drill (SQL on Hadoop)</p></li>
<li><p>Zookeeper (Managing Cluster)</p></li>
<li><p>Oozie (Job Scheduling)</p></li>
<li><p>Flume, Sqoop (Data Ingesting Services)</p></li>
<li><p>Solr &amp; Lucene (Searching &amp; Indexing)</p></li>
<li><p>Ambari (Provision, Monitor and Maintain cluster)</p></li>
<li><p>Giraffe (Working with Graphs)</p></li>
</ul>
<p><img src="./media/media/image3.png" alt="C:\Users\ys8782np\Desktop\HADOOP-ECOSYSTEM-Edureka.png" style="width:5.22847in;height:4.47917in" /></p>
<p><strong>HDFS</strong></p>
<p>HDFS is a java based distributed file system that provides scalable and reliable data storage across multiple nodes in Hadoop clusters. It is scalable, fault-tolerant, distributed storage system that works closely with a wide variety of concurrent data access applications, coordinated by YARN. Having distributed storage and computation across many servers, the combined storage resource can grow linearly with demand while remaining economical at every amount of storage.</p>
<p>HDFS has a master/slave architecture. An HDFS cluster comprises of a single Name Node (master) and multiple Data Nodes (slave nodes). Usually, Name Node executes file system namespace operations like opening, closing and renaming files and directories; it contains attributes like permissions, modification and access and disk space quotas, whereas Data Nodes contains the split data and are responsible for serving read and write requests from the clients. The Data Nodes can also perform block creation, deletion and replication upon instruction from the Name Node.</p>
<p><strong>YARN</strong></p>
<p>YARN allows multiple data processing engines such as interactive SQL, real time streaming, data science and batch processing to process and handle data stored in HDFS. To be precise, YARN acts as a resource manager for different applications to access the distributed data storage. YARN also performs job scheduling.</p>
<p>YARN consists of following components:</p>
<blockquote>
<p><span class="underline"></span></p>
<p><span class="underline">Resource Manager:</span></p>
<p>Resource Manager receives the processing requests, and then passes the parts of request to corresponding Node Manager, which are installed on every data node.</p>
<p>It has a scheduler, which is responsible for allocating resources to the various applications running in the cluster, according to the constraints such a queue capacities and user limits. The scheduler schedules based on the resource requirements of each application.</p>
<p><span class="underline"></span></p>
<p><span class="underline">Node Manager:</span></p>
</blockquote>
<p>Node Managers are responsible for execution of task on every single data node.</p>
<blockquote>
<p><span class="underline"></span></p>
<p><span class="underline">Application Master</span></p>
<p>Application Master is responsible for negotiating appropriate resource containers from the scheduler, tracking their status, and monitoring their progress.</p>
<p><span class="underline"></span></p>
<p><span class="underline">Container</span></p>
</blockquote>
<p>It consists of resources like memory, processing unit and network on a single node.</p>
<p><strong>How does YARN work [5]?</strong></p>
<p><strong>Application of MapReduce in Hadoop with example</strong></p>
<p>The execution of Map and Reduce task in controlled by Job- and Task-trackers. Each cluster-node consists of a single Job Tracker and one Task Tracker. The Job tracker acts as a master and is responsible for scheduling the jobs' component tasks on the slaves (Task Tracker), monitoring them and re-executing the failed tasks. The slaves execute the tasks as directed by the master. Job Tracker resides on Name node and Task Tracker on Data node.</p>
<p><span class="underline">Example text (WordCount_Example.txt): </span></p>
<p><em>Play Golf? Don’t play if the outlook is rain and it’s windy. Don’t play if humidity is high, outlook sunny, and it’s not windy. Otherwise, go play</em></p>
<p><span class="underline">Phase 1: </span></p>
<p>Input Splits:</p>
<p>During this process the input file is divided into chunks of data to be processed by individual mapper. In our example, the text is divided in a chunk of 4 words each.</p>
<p>Mapping:</p>
<p>During mapping process, the output from the input splits is passed to a mapping function to produce output values; job of mapping is to count number of occurrences of each word from input splits and prepare a list in the form of key-value (word, frequency). Please refer to the diagram below and see how each word is seen with frequency of 1.</p>
<p><span class="underline">Phase 2: </span></p>
<p>Shuffling:</p>
<p>This process takes the output of mapping phase. Its task is to consolidate the relevant records from mapping process. In our example, the word “and” is in the file twice, so it is grouped and is seen twice with a frequency of 1 each.</p>
<p>Reducing:</p>
<p>This process combines values from shuffling process and returns a single output value. Considering our example, now the word “and” is seen in the output only once with the frequency of 2.</p>
<p><strong>Installation and configuration of Hadoop on the Cloudera Virtual Machine</strong></p>
<p><em>The installation video is attached in the google drive</em></p>
<p><span class="underline">Download and Install Virtual Box</span></p>
<p>Download link</p>
<p>https://www.virtualbox.org/wiki/Downloads</p>
<p><span class="underline">Download and Install Cloudera Virtual Machine (VM) Image:</span></p>
<p>Download link</p>
<p>https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.4.2-0-virtualbox.zip</p>
<p><span class="underline">Sample text file used for example</span></p>
<p>Available in the drop box with the file named <em>WordCount_Example.txt</em></p>
<p><strong>Testing the installation – running a program in Hadoop</strong></p>
<p>Let us consider the example of wordcount we used earlier for the demonstration on the use of Hadoop. We can run the application using terminal shell</p>
<p><img src="./media/media/image7.png" style="width:5.1375in;height:4.38542in" /></p>
<p><span class="underline">Step 1: </span></p>
<p>Download or generate a file that needs to be used for the application. In our example, a text document titled WordCount_Example.txt is created in the Downloads folder of virtual box. The file can then be transferred to HDFS using the following command:</p>
<p>Code:</p>
<p><em>hadoop fs –copyFromLocal WordCount_Example.txt</em></p>
<p>Screen shot:</p>
<p><img src="./media/media/image8.png" style="width:6.5in;height:1.9875in" /></p>
<p><span class="underline">Step 2: </span></p>
<p>Check if the file is available in the HDFS</p>
<p>Code:</p>
<p><em>hadoop fs –ls </em></p>
<p>Screen shot:</p>
<p><img src="./media/media/image9.png" style="width:6.5in;height:2.77569in" /></p>
<p><span class="underline">Step 3: </span></p>
<p>We can use jar command to run a program in hadoop from a jar file. The jar file is in usr/jars/hadoop-examples.jar, and we are going to use wordcount program on the text file we have in the file system.</p>
<p>Syntax:</p>
<p>hadoop jar /usr/jars/hadoop-examples.jar wordcount &lt;input&gt; &lt;output&gt;</p>
<p>Code:</p>
<p><em>hadoop jar /usr/jars/hadoop-examples.jar wordcount WordCount_Example.txt Eout</em></p>
<p>Screen shot:</p>
<p><img src="./media/media/image10.png" style="width:6.5in;height:0.88264in" /></p>
<p>The MapReduce application will now perform the map and reduce task. The process can be monitored on the screen. In the screen below, we can see that the input has been split and is awaiting the map and reduce phase (both at 0%)</p>
<p><img src="./media/media/image11.png" style="width:6.5in;height:2.02431in" /></p>
<p>Once Map phase completes, Reduce phase starts</p>
<p><img src="./media/media/image12.png" style="width:6.5in;height:2.02292in" /></p>
<p>After the completion of Reduce phase, we get the summary of task completed by the application</p>
<p><img src="./media/media/image13.png" style="width:6.5in;height:4.87847in" /></p>
<p><span class="underline">Step 4: </span></p>
<p>Check if the output directory is created</p>
<p>Code:</p>
<p><em>hadoop fs –ls</em></p>
<p>Screen shot:</p>
<p><img src="./media/media/image14.png" style="width:6.5in;height:1.58264in" /></p>
<p><span class="underline"></span></p>
<p><span class="underline">Step 5: </span></p>
<p>Open the directory to check the files in the directory. There are two files in the directory:</p>
<p>Eout/_SUCCESS and</p>
<p>Eout/part-r-00000.</p>
<p>The Eout/_SUCCESS means that the wordcount job ran successfully. The other file is the output generated by the process.</p>
<p><img src="./media/media/image15.png" style="width:6.5in;height:2.10972in" /></p>
<p><span class="underline">Step 6: </span></p>
<p>The output file can now to copied to local file system from HDFS and view it.</p>
<p>Code:</p>
<p><em>hadoop fs –copyToLocal Eout/part-r-0000 Output.txt</em></p>
<p>Screen shot:</p>
<p><img src="./media/media/image16.png" style="width:6.5in;height:0.79583in" /></p>
<p><span class="underline">Step 7: </span></p>
<p>We can now read the output file. The file contains the word and the frequency of the word in the file we sent in as an input.</p>
<p>Code:</p>
<p><em>more Output.txt</em></p>
<p>Screen shot:</p>
<p><img src="./media/media/image17.png" style="width:5.75in;height:3.69097in" /></p>
<p><strong>Exercise</strong></p>
<ol type="1">
<li><p>Download the text file from the following link (instructor could provide their own text file, the specified file in the link is used just as an example):</p></li>
</ol>
<blockquote>
<p>https://www.w3.org/TR/PNG/iso_8859-1.txt</p>
</blockquote>
<ol start="2" type="1">
<li><p>Copy the file in HDFS</p></li>
<li><p>Run wordcount program using MapReduce</p></li>
<li><p>Explain each phase of MapReduce and submit the file output to the instructor</p></li>
</ol>
<p><strong><br />
</strong></p>
<p><strong>Websites of relevance</strong></p>
<ol type="1">
<li><p>What is Big Data and why it matters. (n.d.). Retrieved July 1, 2018, from https://www.sas.com/en_us/insights/big-data/what-is-big-data.html</p></li>
<li><p>MapReduce Tutorial. (n.d.). Retrieved July 1, 2018, from https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</p></li>
<li><p>Sinha, S. (2018, July 17). Hadoop Ecosystem | Hadoop Tools for Crunching Big Data | Edureka. Retrieved July 1, 2018, from https://www.edureka.co/blog/hadoop-ecosystem</p></li>
<li><p>HDFS Architecture Guide. (n.d.). Retrieved July 1, 2018, from https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html</p></li>
<li><p>Subramaniam, A. (2018, July 17). Apache Hadoop YARN | Introduction to YARN Architecture | Edureka. Retrieved July 1, 2018, from https://www.edureka.co/blog/hadoop-yarn-tutorial/</p></li>
<li><p>Big Data. (n.d.). Retrieved July 1, 2018, from https://words.sdsc.edu/words-data-science/big-data</p></li>
<li><p>Apache Hadoop HDFS. (n.d.). Retrieved July 1, 2018, from https://hortonworks.com/apache/hdfs/</p></li>
<li><p>Apache Hadoop YARN. (n.d.). Retrieved July 1, 2018, from https://hortonworks.com/apache/YARN/</p></li>
<li><p><a href="https://computing.llnl.gov/tutorials/parallel_comp/"><span class="underline">https://computing.llnl.gov/tutorials/parallel_comp/</span></a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Beowulf_cluster"><span class="underline">https://en.wikipedia.org/wiki/Beowulf_cluster</span></a></p></li>
<li><p><a href="https://www.top500.org/"><span class="underline">https://www.top500.org/</span></a></p></li>
</ol>
