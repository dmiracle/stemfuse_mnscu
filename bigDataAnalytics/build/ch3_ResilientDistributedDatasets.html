<!DOCTYPE html>
<html>
  <head>
  <title>Visualization in R for Qualitative Variables</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
  <meta name="person" content="" />
  <meta name="course" content="" />


<!--[if lt IE 9]>
	<link href="/shared/metroStyleIE8.css" type="text/css" rel="stylesheet">
  <![endif]-->

  
  <link href="inc/metroStyle.css" type="text/css" rel="stylesheet">


  <!--LaTex Includes
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.4.1/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">
-->

  <style>
  
  .warning {
    visibility: hidden;
  }
  
  </style>
</head>

<body>
<p style="text-align: center;"><a name="top"></a></p>
<div id="pageContainer">
<div class="toTop"><a href="#top">Top of Page &gt;&gt;</a></div>

<div id="mainContent">
<div class="warning"><span style="color: #cf2a27;"><strong>Do not delete... If you delete into this warning block, Control + Z to Undo... Do not delete</strong></span></div>

<!-- PASTE HTML HERE-->
<h1 id="resilient-distributed-dataset-(rdd)">Resilient Distributed Dataset (RDD)</h1>
<p>Spark's architecture is designed around a data structure called the Resilient Distributed Dataset (RDD).</p>
<p>RDDs may be considered as specialized collections in the Java sense. Like a hash map in Java, RDDs can store primitive and object data, including other lists, and can be stored to disk as serialized object files.</p>
<p>As a key-value data structure, the RDDs are partitioned according to the number of distribution targets. You can think of each partition as representing a sublist from a master list of data. If you are unfamiliar with HashMap or key-value oriented data structures, consider these like parallel arrays (associative arrays) with one array used to stored the index (or key) and the other array used to store the value per the same index.</p>
<p>Each data set in the RDD may be considered a record, or it might be another object.</p>
<p>The RDD is immutable. That means that any transformation entails a new RDD; the original is maintained. And since each node maintains the full original RDD, the failure of one node means that its RDD partition can be applied to other nodes. Hence, the notion of “Resilient.”</p>
<p>RDDs accommodate map and reduce operations. The an RDD key can be used for a search or to accumulate a series of values from worker nodes.</p>
<p>For reduce operations, Spark has combine functions. Combine functions include sum, but exclude other that do not correctly evaluate mathematics after data is aggregated from nodes.</p>
<p>RDD operations:</p>
<ul>
<li><p>Transformations − create a new RDD from an existing RDD. Ex: map, filter, groupBy, join</p></li>
<li><p>Action − effect RDD elements and return result to driver. Ex: collect</p></li>
</ul>
<p>Here is an example of join:</p>
<pre><code>a = sc.parallelize([(&quot;goat&quot;, 1), (&quot;pig&quot;, 3), (&quot;cow&quot;, 5)])
b = sc.parallelize([(&quot;goat&quot;, 2), (&quot;pig&quot;, 4), (&quot;cow&quot;, 6)])

ab = a.join(b)
ab_here = ab.collect()
print &quot;Set a joined to set b: %s&quot; % (ab_here)
→ (&#39;Set a joined to set b: %s&#39;, [(&#39;goat&#39;, (1, 2)), (&#39;cow&#39;, (5, 6)),
(&#39;pig&#39;, (3, 4))])
</code></pre>
<p>Important notes regarding these RDDs:</p>
<ul>
<li>Have a key-value structure.</li>
<li>Join occurs on common keys.</li>
</ul>
<p>Therefore, if the sets have no commond keys, the RDD resulting from a join will be empty. For example, if we had a third set with only one common key, observe the join:</p>
<pre><code>c = sc.parallelize([(&quot;goat&quot;, 7), (&quot;lamb&quot;, 1), (&quot;chicken&quot;, 3)])
a.join(c).collect()        →        [(&#39;goat&#39;, (1, 7))]
</code></pre>
<p>Check that the RDD is cached:</p>
<pre><code>ab.persist().is_cached       →       True
</code></pre>
<p>More:</p>
<p><a href="https://spark.apache.org/docs/latest/api.html">https://spark.apache.org/docs/latest/api.html</a></p>
<p><a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html">https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html</a></p>
<h2 id="rdd-partitioning">RDD partitioning</h2>
<p>Partitioning of RDDs can be done using HashPartitioner or CustomPartitioner.</p>
<p>HashPartitioner:</p>
<ul>
<li>Spark's defaut partitioner</li>
<li>The key is from Java’s Object.hashcode() method. This will provide each element of an RDD with a distinct hash if the associated object is distinct. Objects that are the same will have the same hash (Java example: <code>“key”.hashCode().equals(“key”.hashCode()) → true</code>)</li>
</ul>
<p>RangePartitioner:</p>
<ul>
<li>Creates equally sized partitions on sortable records.</li>
<li>Initially sorts according key.</li>
</ul>
<p>CustomPartitioner:</p>
<ul>
<li>Just extend Spark's Partitioner class.</li>
</ul>
<h2 id="python-dictionaries-versus-rdds">Python dictionaries versus RDDs</h2>
<p>An efficient way to compute modes is using associative-arrays, also known as dictionaries. Dictionaries are associated (key : value) pairs. Unlike strings and lists, dictionaries are a group of Python collections known as non-sequential collections (as opposed to sequential). Like lists, dictionaries are mutable.</p>
<p>For example:</p>
<pre><code>times = {&#39;Owens&#39; : 10.1, &#39;Smith&#39; : 10.5, &#39;Franks&#39; : 10.7 }
</code></pre>
<p>Or a string that concatenates two strings for a string-int pair:</p>
<pre><code>&gt;&gt;&gt; prices[“Fall” + “, Boston”] = 21
&gt;&gt;&gt; prices[“Spring” + “, Chicago”] = 23
&gt;&gt;&gt; prices[“Summer” + “, NYC”] = 22
&gt;&gt;&gt; prices[“Fall” + “, Boston”] 
21
</code></pre>
<p>RDDs, like Python dictionaries, have their own methods. Of particular note are the methods to get the keys and values. In Python there is the form dName.keys() and dName.values(). [equiv in Spark ]</p>
<p>The method items() returns a tuple from a dictionary. Tuples are similar to lists, but are immutable. They can have various data types.</p>
<pre><code>&gt;&gt;&gt; times.items()
dict_items([(&#39;Franks&#39;, 10.7), (&#39;Owens&#39;, 10.1), (&#39;Smith&#39;, 10.5)]) 

&gt;&gt;&gt; times.get(&quot;Owens&quot;) 
10.1 
</code></pre>
<p>Suppose there are multiple values in a dictionary having the same key. Can you devise a way to compute the average of these values? Here is an example that combines two “associative arrays” with dictionaries. Based on this example, it may come as no surprise that dictionaries are also referred to as associative arrays.</p>
<pre><code>import decimal 

# some times 
times = [ &quot;Owens:10.1&quot;, &quot;Smith:10.5&quot;, &quot;Owens:10.8&quot;, \
        &quot;Frank:10.7&quot;, &quot;Owens:10.2&quot;, &quot;Lewis:10.4&quot; ]   

# define some more dictionaries for holding stats 
total = { } 
mode = { }

# useful key and value arrays 
key = [] 
value = [] 

# initialize the keys of the stat dictionaries 

for i in times: 
    pair = i.split(&quot;:&quot;) 
    key = pair[0] 
    total[key] = 0 
    mode[key] = 0 

for i in times:
    pair = i.split(&quot;:&quot;)        
    key = pair[0]              
    value = pair[1]              

    total[key] += decimal.Decimal(value)            
    mode[key] = mode[key] + 1 

# output average time for Owens only 
mean = total[&#39;Owens&#39;] / mode[&#39;Owens&#39;] 
print(mean) 

print &quot;Owen&#39;s average time is: %0.2f seconds over %i races.&quot; % (mean, mode\[&#39;Owens&#39;\])  

→ Owen&#39;s average time is: 10.37 and number of races is 3
</code></pre>
<p>Converting the above program to PySpark, we use map and groupByKey operations. For example:</p>
<pre><code>times = [(&#39;Owens&#39;,10.1), (&#39;Smith&#39;,10.5), (&#39;Owens&#39;,10.8), (&#39;Frank&#39;,10.7), (&#39;Owens&#39;,10.2), (&#39;Lewis&#39;,10.4)]

sc.parallelize(times).groupByKey().mapValues(lambda a: sum(a) / len(a)).collect()
→
[(&#39;Smith&#39;, 10.5), (&#39;Frank&#39;, 10.7), (&#39;Lewis&#39;, 10.4), (&#39;Owens&#39;, 10.36)]
</code></pre>
<p>As you can see, the PySpark code is brief relative to regular Python.</p>
<p>RDD APIs:</p>
<p><a href="https://spark.apache.org/docs/latest/api/">https://spark.apache.org/docs/latest/api/</a></p>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>

</div>
</div>
</body>
</html>