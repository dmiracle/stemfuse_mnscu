<h1 id="the-map-reduce-paradigm">The map-reduce paradigm</h1>
<p>When faced with huge data sets we are motivated to avoid serial in design and execution, as it can tax patience. Instead, we try to break a workload into parts that can be processed independently using multiple machines. Such parallelization may or may not be feasible given the problem at hand, and we have to think about what problems can be solved in asynchronous task parallel and data parallel fashion.</p>
<p>Processing of partitioned data on cluster nodes takes place independently and concurrently. Results can be consolidated on a master node according to the keys of key-value pairings in the data.</p>
<p>While the map reduce concept goes back a to the time when Lisp was at is zenith, Google managed to win a patent for its version, spelled MapReduce (USPTO patent #7,650,331).</p>
<p>The map-reduce paradigm supports parallel computing. The map function as applied to each value of a data set is readily performed in parallel, as the the value and the function are independent. However, in the context of distributed processing, a reduce operation will depend on a completed map operation.</p>
<h2 id="map-and-reduce-definitions">Map and Reduce Definitions</h2>
<p>Fundamentally, the map and reduce functions each take as parameters a data set and a function, which is then applied to each element of data. The map function then returns a transformed data set. The reduce function returns an aggregate of the the data set.</p>
<ul>
<li><p><strong>Map</strong> (definition 2): index and partition input data into a subset for each compute node. In tree structured clusters, worker nodes are masters of subordinate nodes.</p></li>
<li><p><strong>Reduce</strong> (definition 2): the master node consolidates compute node output into one data set. Reduction may entail sorting, averaging, filtering, etc.</p></li>
</ul>
<p>In Apache Spark, map and reduce work on local data, and the map-reduce paradigm is injected with “shuffle” and “sort” operations. We have the following definitions:</p>
<ul>
<li><p><strong>Map</strong> (definition 3): Worker nodes apply the map function to their respective local data, maintaining results in temporary storage or memory. Map operations occur on each partition element and include transforming or filtering.</p></li>
<li><p><strong>Shuffle</strong>: The transfer of values from the mapper to the reducer. Worker nodes are assigned a partition of data associated with a sorted range of keys. The keys are generated by a dedicated key-value map function.</p></li>
<li><p><strong>Sort</strong>: a dependency for shuffle, with ordering by keys, then value.</p></li>
<li><p><strong>Reduce</strong> (definition 3): Worker nodes aggregate (sum, count) all partition values by key.</p></li>
</ul>
<p>The pseudo code for a map:</p>
<pre><code>map(key, value) → list(key, value&#39;)
</code></pre>
<p>The pseudo code for a map:</p>
<pre><code>reduce(key, list(value&#39;)) → (key, value&#39;&#39;)
</code></pre>
<p>Apache Hadoop's implementation of Google's MapReduce entails additional operations such as split and combine. The following illustration provides some clarity (Source: <a href="https://www.mssqltips.com/sqlservertip/3222/big-data-basics--part-5--introduction-to-mapreduce/">https://www.mssqltips.com/sqlservertip/3222/big-data-basics--part-5--introduction-to-mapreduce/</a> )</p>
<p><img src="./ch2_Map-Reduce_Paradigm/2f61b138dc588dba6c7b6d05f6f6876f33deb9eb.png" /></p>
<p>In the past decade, the general map-reduce concept has been convoluted by various implementations, including complex ones like that of MapReduce for Hadoop. Apache Spark does not require Hadoop Google's MapReduce. Spark employs the map-reduce paradigm in partitioning RDDs and applying transformation to data in dedicated areas of distributed memory.</p>
