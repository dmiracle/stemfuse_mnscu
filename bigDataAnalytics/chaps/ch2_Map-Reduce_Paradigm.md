# The map-reduce paradigm

When faced with huge data sets we are motivated to avoid serial in
design and execution, as it can tax patience. Instead, we try to break a
workload into parts that can be processed independently using multiple
machines. Such parallelization may or may not be feasible given the
problem at hand, and we have to think about what problems can be solved
in asynchronous task parallel and data parallel fashion.   

Processing of partitioned data on cluster nodes takes place
independently and concurrently. Results can be consolidated on a master
node according to the keys of key-value pairings in the data. 

While the map reduce concept goes back a to the time when Lisp was at is
zenith, Google managed to win a patent for its version, spelled
MapReduce (USPTO patent \#7,650,331).     

The map-reduce paradigm supports parallel computing. The map function as
applied to each value of a data set is readily performed in parallel, as
the the value and the function are independent. However, in the context
of distributed processing, a reduce operation will depend on a completed
map operation. 

## Map and Reduce Definitions

Fundamentally, the map and reduce functions each take as parameters a
data set and a function, which is then applied to each element of data.
The map function then returns a transformed data set. The reduce
function returns an aggregate of the the data set. 

- **Map** (definition 2): index and partition input data into a subset for each compute node. In tree structured clusters, worker nodes are masters of subordinate nodes.      

- **Reduce** (definition 2): the master node consolidates compute node output into one data set. Reduction may entail sorting, averaging, filtering, etc.   
 
In Apache Spark, map and reduce work on local data, and the map-reduce
paradigm is injected with “shuffle” and “sort” operations. We have the
following definitions:

  - **Map** (definition 3): Worker nodes apply the map function to their respective local data, maintaining results in temporary storage or memory. Map operations occur on each partition element and include transforming or filtering.

  - **Shuffle**: The transfer of values from the mapper to the reducer. Worker nodes are assigned a partition of data associated with a sorted range of keys. The keys are generated by a dedicated key-value map function. 

  - **Sort**: a dependency for shuffle, with ordering by keys, then value.

  - **Reduce** (definition 3): Worker nodes aggregate (sum, count) all partition values by key.

The pseudo code for a map:
```
map(key, value) → list(key, value')
```
The pseudo code for a map:
```
reduce(key, list(value')) → (key, value'')
```
Apache Hadoop's implementation of Google's MapReduce entails additional
operations such as split and combine. The following illustration
provides some clarity (Source:
<https://www.mssqltips.com/sqlservertip/3222/big-data-basics--part-5--introduction-to-mapreduce/>
)

![](.//Pictures/10000201000002BC000001FEE4968C78FBF52BBF.png)

In the past decade, the general map-reduce concept has been convoluted
by various implementations, including complex ones like that of
MapReduce for Hadoop. Apache Spark does not require Hadoop Google's
MapReduce. Spark employs the map-reduce paradigm in partitioning RDDs
and applying transformation to data in dedicated areas of distributed
memory. 
