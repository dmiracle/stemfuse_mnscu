<h1 id="functional-programming">Functional Programming</h1>
<h2 id="functional-programming-in-pyspark">Functional Programming in PySpark</h2>
<p>Various programming languages implement the functional paradigm, including Java via Lambdas, D, Python and Scala.</p>
<p>Relative to standard procedural languages such as C and Java, functional programming allows to program with relatively concise statements that reflect mathematical expressions (accordance with formal mathematical provability). This is because functional programming breaks a problem into specialized functions connected by input and output.</p>
<p>Modularity is a byproduct of breaking a problem into is smallest parts, each represented by a function. Programs that are comprised of many distinct small functions, each performing a single task, are more maintainable than a large complicated function (effectively a program). In turn, a benefit of the modularity afforded by functional constraints includes quicker prototyping.</p>
<p>Formally, functional programming entails immutable objects, and if a language is “purely functional,” then there are no side effects (that alter some system state apart from the function itself). Hence, a function's output always depends on an explicit input. This also means that objects are not changed by a function. Instead, a new object is created for output, as is the case with Spark's central data structure, the Resilient Distributed Dataset or RDD.</p>
<p>Functions in a functional programming language may be “higher order functions,” meaning that they accept other functions as arguments and are able to return a function. Typical examples of higher order functions include map, reduce and filter. Such higher order functions exist in PySpark.</p>
<p>Because functional languages put constraints on side effects, functions and their processes can operate independently, which is required for asynchronous parallel operations. That way the processes of various functions do not contest the same objects or system resources, which would slow processing.</p>
<h3 id="lazy-evaluation">Lazy evaluation</h3>
<p>Lazy evaluation is another characteristic of functional programming. Lazy evaluation reserves the evaluation of an expression until it is needed. This facilitates modularization since it may not be known which function or expression should ultimately be engaged. This goes hand-in-hand with being side-effect free, so that program behavior is not affected by the order of evaluation.</p>
<h3 id="lambda-expressions">Lambda expressions</h3>
<p>In several languages that support functional programming, a facility called lambdas that accommodates small user defined, or anonymous, functions that can be passed as parameters to other functions.</p>
<p>Python has a functional programming library called functools. PySpark readily accommodates functional programming.</p>
<p>In Python, a lambda expression combines parameters into an “anonymous function” such as:</p>
<pre><code>lambda x, y: x * y
</code></pre>
<p>We can assign this lambda to a variable such as:</p>
<pre><code>product = lambda x, y: x * y
</code></pre>
<p>and affect the lambda as:</p>
<pre><code>print(product(2,3))        →         6
</code></pre>
<p>In other words, we assigned the anonymous function to “product” and passed it as a parameter to the print() function. However, we need not even name the function. For example:</p>
<pre><code>result = otherFunction(lambda x, y: x * y)
</code></pre>
<p>This arrangement enforces narrow functionality. This is unlike defining a using the “def” keyword in Python, which encourages line-after-line of alternative actions for the function to take.</p>
<p>To use conditional expressions, we can have:</p>
<pre><code>fn = lambda L, R: (&quot;Go Left&quot;, L, R) if L &gt; 5 and R &lt; 10 else (&quot;Go Right&quot;, L, R)
print(fn(5,15))        →        (&#39;Go Right&#39;, 5, 15)
</code></pre>
<h2 id="higher-order-functions">Higher order functions</h2>
<p>Higher order functions (aka “functors”) accept other functions as arguments and are able to return a function. Examples of higher order functions include map, filter, flatMap and reduce.</p>
<h4 id="map"><code>map</code></h4>
<p>The map function applies another function to a list of values. Its output is a new list of transformed elements.</p>
<p>map takes two arguments:</p>
<pre><code>map(function, list)
</code></pre>
<p>For example, suppose we have a list of exam grades:</p>
<pre><code>grades = [30, 35, 55, 70, 45, 60]
</code></pre>
<p>To scale all exam grades up by 10 points, map a lambda as follows:</p>
<pre><code>grades_adj = list(map(lambda a: a + 10, grades))
list(grades_adj)        →        [40, 45, 65, 80, 55, 70]
</code></pre>
<h4 id="reduce"><code>reduce</code></h4>
<p>Like map, reduce applies a function to a list of values. Unlike map, the output of reduce is an aggregate of the list. The operation of reduce is pairwise such that for elements a, b, the pairing “a + b, b” creates a sum aggregate. For example:</p>
<pre><code>ints = [2, 4, 6, 8]
sum = lambda a, b: a + b
avg = reduce(sum, ints)/len(ints)
print(avg)        →        20
</code></pre>
<h4 id="filter"><code>filter</code></h4>
<p>Used to filter data, conditionally allowing some data to pass without modifying it. Hence, the resulting list may be shorter than the input list.</p>
<pre><code>ok = lambda a: a &gt; 0
ints = [2, -4, 6, -8]
pos_ints = filter(ok, ints)
print(pos_ints)        →        [2, 6]
</code></pre>
<h4 id="flatmap"><code>flatMap</code></h4>
<p>The map function may yield multiple sub lists. If a single “flattened” list is preferred, then use <code>flatMap</code>.</p>
<p>For example, in PySpark suppose the following</p>
<pre><code>sc.parallelize(grades).map(lambda a: [a, a + 10]).collect() 
→ [[30, 40], [35, 45], [55, 65], [70, 80], [45, 55], [60, 70]]
</code></pre>
<p>Using flatMap we get:</p>
<pre><code>sc.parallelize(grades).flatMap(lambda a: [a, a + 10]).collect() 
→ [30, 40, 35, 45, 55, 65, 70, 80, 45, 55, 60, 70]
</code></pre>
<p>with results “flattened” into one single list. So, given a couple sets of grade provided as:</p>
<p>Similarly, given nested lists such as:</p>
<pre><code>grades = [[20, 40, 60, 80], [10, 30, 50, 70]]
</code></pre>
<p>We can flatten the list into a single list as follows:</p>
<pre><code>sc.parallelize(grades).flatMap(lambda a: a).collect() 
→ [20, 40, 60, 80, 10, 30, 50, 70]
</code></pre>
<p>That is enough of a functional programming overview for this module. For more functional programming material and Python basics, please visit the other Python and Functional Programming modules.</p>
<p>The concepts of map and reduce as seen in Python evolve in the distributed context of the Spark environment, where map performs transformations while another function “collect” is responsible for retrieving transformed elements.</p>
<p>Related Internet resources:</p>
<p><a href="https://en.wikipedia.org/wiki/Functional_programming">https://en.wikipedia.org/wiki/Functional_programming</a></p>
<p><a href="https://en.wikipedia.org/wiki/Currying">https://en.wikipedia.org/wiki/Currying</a></p>
<p><a href="https://docs.python.org/2.7/library/functools.html">https://docs.python.org/2.7/library/functools.html</a></p>
<p><a href="https://en.wikipedia.org/wiki/Higher-order_function">https://en.wikipedia.org/wiki/Higher-order_function</a></p>
<h2 id="exercises">Exercises</h2>
<ol>
<li>Using Python, given a list of pets: <code>pets = ['dogs', 'cats', 'birds', 'fish']</code> Use the map function to print the character count comprising each pet type.</li>
<li>Given the list: <code>nums = [1, 2, 3, 4, 5, 6, 7, 8, 9]</code> Devise a lambda function to yield a list of only the odd integers.</li>
<li>Using Python, given a list of pairs, pets with their associated count, as (pet, count): <code>pets = [('dogs', 5), ('cats', 3), ('birds', 7), ('fish', 4)]</code> Devise lambda functions to do the following:
<ul>
<li>Sort the pets alphabetically by type.</li>
<li>Sort the pets numerically by count.</li>
<li>Retrieve the total count of all pets.</li>
<li>(Hint: Lists contain a sort function.)</li>
</ul></li>
</ol>
<p>Submit your code and output from executing your programs.</p>
