<h1 id="spark-sql">Spark SQL</h1>
<p>Spark SQL allows RDD transforms using Structured Query Language (SQL). Ironically, it does not require a database engine such as SQL Server or Oracle be accessed, though it can do that as well. The flexibility of Spark SQL is that data can reside in semi-structured text files, such as JSON.</p>
<p>The Spark SQl API: <a href="https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html">https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html</a></p>
<p>Given the file SPY.csv (5000 records with 7 fields each), I ran the following tasks:</p>
<pre><code>import csv
aFile = &quot;/home/.../SPY.csv&quot;
rdd = sc.textFile(aFile)
rdd_result = rdd.mapPartitions(lambda fields: csv.reader(fields))
</code></pre>
<p>The following example program wordcount.py ships with Spark. It is modified here for style.</p>
<pre><code>from __future__ import print_function
import sys
from operator import add
from pyspark.sql import SparkSession

if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 2:
        print(&quot;Usage: wordcount &lt;file&gt;&quot;, file=sys.stderr)
        exit(-1)
    spark = SparkSession.builder.appName(&quot;PythonWordCount&quot;).getOrCreate()
    
    aFile = sys.argv[1]
    lines = spark.read.text(aFile).rdd.map(lambda r: r[0])
    counts = lines.flatMap(lambda x: x.split(&#39;,&#39;)).map(lambda x: (x, 1)).reduceByKey(add)
    output = counts.collect()

    for (word, count) in output:
        print(&quot;%s: %i&quot; % (word, count))

    spark.stop()
</code></pre>
<p>That program was run on a 3-node cluster from the command-line as:</p>
<pre><code>$SPARK_HOME/bin/spark-submit \
$SPARK_HOME/examples/src/main/python/wordcount.py \
        /home/.../SPY.csv
</code></pre>
<p>Here is another example that reads in a csv file:</p>
<pre><code>from pyspark import SparkContext
from pyspark.sql import SQLContext

aFile = &quot;/home/.../data/SPY.txt&quot;
dat = spark.read.csv(aFile)
# dat.dtypes
# dat.printSchema()

dat.select(&quot;*&quot;).show(10)

+----------+------+------+------+------+---------+------+
|   _c0|    _c1|    _c2|    _c3|    _c4|    _c5|    _c6|
+----------+------+------+------+------+---------+------+
|2014-06-13|193.92|194.32|193.30|194.13| 81991900|194.13|
|2014-06-12|194.69|194.80|193.11|193.54|106350000|193.54|
|2014-06-11|194.90|195.12|194.48|194.92| 68772000|194.92|
|2014-06-10|195.34|195.64|194.92|195.60| 57129000|195.60|
|2014-06-09|195.35|196.05|195.17|195.58| 65119000|195.58|
|2014-06-06|194.87|195.43|194.78|195.38| 78696000|195.38|
|2014-06-05|193.41|194.65|192.70|194.45| 92103000|194.45|
|2014-06-04|192.47|193.30|192.27|193.19| 55529000|193.19|
|2014-06-03|192.43|192.90|192.25|192.80| 65047000|192.80|
|2014-06-02|192.95|192.99|191.97|192.90| 64656000|192.90|
+----------+------+------+------+------+---------+------+

only showing top 10 rows
</code></pre>
<p>A query using SQL's “where” clause to select all data in the year 2012:</p>
<pre><code>dat.where(s._c0.rlike(&#39;2012&#39;)).show()

+----------+------+------+------+------+---------+------+
|   _c0|    _c1|    _c2|    _c3|    _c4|    _c5|    _c6|
+----------+------+------+------+------+---------+------+
|2012-12-31|139.66|142.56|139.54|142.41|243935200|138.98|
|2012-12-28|140.64|141.42|139.87|140.03|148806700|136.66|
|2012-12-27|141.79|142.08|139.92|141.56|167920600|138.15|
|2012-12-26|142.64|142.71|141.35|141.75|106947700|138.33|
|2012-12-24|142.48|142.56|142.19|142.35| 53874600|138.92|
|2012-12-21|142.17|144.09|141.94|142.79|245883800|139.35|
|2012-12-20|144.38|145.14|143.98|145.12|168487000|140.63|
|2012-12-19|145.53|145.58|144.24|144.29|150895400|139.82|
|2012-12-18|144.00|145.50|143.79|145.37|177762800|140.87|
|2012-12-17|142.47|143.85|142.43|143.77|143238200|139.32|
|2012-12-14|142.32|142.58|141.88|142.10|137701700|137.70|
|2012-12-13|143.42|143.83|142.27|142.63|135715000|138.21|
|2012-12-12|144.00|144.55|143.31|143.51|145880100|139.06|
|2012-12-11|143.06|144.11|142.99|143.44|152570400|139.00|
|2012-12-10|142.21|142.81|142.15|142.47| 98840700|138.06|
|2012-12-07|142.53|142.69|141.67|142.41|108726400|138.00|
|2012-12-06|141.37|142.04|141.16|141.98|103220600|137.58|
|2012-12-05|141.37|142.16|140.37|141.50|147300500|137.12|
|2012-12-04|141.44|141.87|140.87|141.25|127512200|136.88|
|2012-12-03|142.80|142.92|141.34|141.45|124656300|137.07|
+----------+------+------+------+------+---------+------+

only showing top 20 rows
</code></pre>
<p>In order to use standard SQL query strings, first define a temporary table:</p>
<pre><code>dat.registerTempTable(&quot;SPY&quot;)
</code></pre>
<p>Now query as follows:</p>
<pre><code>sql(&quot;select sum(_c5) from SPY where _c0 like &#39;2012%&#39;&quot;).show()

+------------------------+
|sum(CAST(_c5 AS DOUBLE))|
+------------------------+
|          3.58327342E10|
+------------------------+
</code></pre>
<p>Spark SQL can connect to to a database via the OBDC library. Keep in mind that if all compute nodes connect to a single database server, that server may become a bottleneck. Alternatively, you can use serverless SQLite (all nodes will need their own installation of SQLite).</p>
<p>Invoke PySpark session with SQLite:</p>
<pre><code>pyspark --driver-class-path .:sqlite-jdbc-3.8.11.2.jar
</code></pre>
<p>Create a data frame:</p>
<pre><code>df = sqlContext.read.format(&#39;jdbc&#39;). \
    options(url=&#39;jdbc:sqlite:/home/.../data/Chinook\_Sqlite.sqlite&#39;, \ 
    dbtable=&#39;employee&#39;,driver=&#39;org.sqlite.JDBC&#39;).load()
</code></pre>
<p>Test:</p>
<pre><code>df.printSchema()

→ 

root

|-- EmployeeId: integer (nullable = false)
|-- LastName: string (nullable = true)
|-- FirstName: string (nullable = true)
|-- Title: string (nullable = true)
|-- ReportsTo: integer (nullable = true)
|-- BirthDate: date (nullable = true)
|-- HireDate: date (nullable = true)
|-- Address: string (nullable = true)
|-- City: string (nullable = true)
|-- State: string (nullable = true)
|-- Country: string (nullable = true)
|-- PostalCode: string (nullable = true)
|-- Phone: string (nullable = true)
|-- Fax: string (nullable = true)
|-- Email: string (nullable = true)
</code></pre>
<h2 id="exercise">Exercise</h2>
<p>Note that Spark and SQL have some common methods. Using both Spark's RDD API methods and Spark SQL to create standard SQL Query strings, write PySpark code for the following operations: group by, count, sort, filter, union, intersection.</p>
<p>Apply your code to the Chinook database at</p>
<p><a href="https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite">https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite</a></p>
<p>See also the Spark method APIs at:</p>
<p><a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html">https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html</a></p>
<p>Based on this exercise, you should answer the following questions:</p>
<ul>
<li><p>Which approach is more costly in terms of coding times?</p></li>
<li><p>Which approach is likely to be more time consuming in processing Big Data?</p></li>
</ul>
<p>Upload your code and the first 10 lines of test output for each RDD and SQL method.</p>
