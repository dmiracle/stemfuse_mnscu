<!DOCTYPE html>
<html>

<head>
  <title>Information Theory</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
  <meta name="person" content="" />
  <meta name="course" content="" />


  <!--[if lt IE 9]>
	<link href="/shared/metroStyleIE8.css" type="text/css" rel="stylesheet">
  <![endif]-->


  <link href="inc/metroStyle.css" type="text/css" rel="stylesheet">
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.js" type="text/javascript"></script>
  <script src="inc/metroScript.js" type="text/javascript"></script>

  <!--LaTex Includes
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.4.1/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">
  -->

  <style>

    .warning {
    visibility: hidden;
  }
  #leftSideBar {
      background-color: #003C66;
      background-repeat: no-repeat;
      background-position: top center;
      background-size: 100%;
      min-height: 100%;
      width: 200px;
      float: left;
      padding-top: 10px;
      padding-right: 1%;
      padding-left: 1%;
      font-size: 1em;
      line-height: 100%;
      color: white;
  }
  
  #leftSideBar a {
      color: white;
      text-decoration: none;
  }
  
  #leftSideBar li {
      margin:0 0 1em 0;
  }
  
  </style>
</head>

<body>
  <p style="text-align: center;"><a name="top"></a></p>
  <div id="pageContainer">
    <div class="toTop"><a href="#top">Top of Page &gt;&gt;</a></div>
    <div id="leftSideBar">
      <img src="images/ITCoE_white200px.png">
      <!---
  <h2 style="color:white;">Sections</h2>
-->
      <hr style="color: white;">
      <ul>
        <li>
          <h2 style="color: white;">Sections</h2>
        </li>
      </ul>
      <hr style="color: white;">
      <ul>
        <!--- Place sidebar links here like this:
    <li><a href="#learning-objectives">Learning Objectives</a></li>
-->
        <li><a href="#instructor-overview-and-objective">Instructor Overview and Objective</a></li>
        <li><a href="#table-of-contents">Table of Contents</a></li>
        <li><a href="#overview-of-information-theory">1. Overview of Information Theory</a></li>
        <li><a href="#entropy">2. Entropy</a></li>
        <li><a href="#transmission-of-messages">3. Transmission of Messages</a></li>
        <li><a href="#noise,-error-detection,-and-correction">4. Noise, Error Detection, and Correction</a></li>
        <li><a href="#data-compression-and-huffman-encoding">5. Data Compression and Huffman Encoding</a></li>
        <li><a href="#additional-references">6. Function</a></li>
        <!--- end sidebar -->
      </ul>
      <div id="printDiv" style="text-align: center;"><input onclick="window.print(); return false;" alt="Print" src="https://metrostate.learn.minnstate.edu/shared/Admin/Templates/Templates/../../../images/icons/print.png"
          width="32" type="image" height="32" /></div>
      <hr style="color: white;">
      <img src="images/mnstate_white200px.png">
    </div>
    <div id="mainContent">
      <div class="warning"><span style="color: #cf2a27;"><strong>Do not delete... If you delete into this warning
            block, Control + Z to Undo... Do not delete</strong></span></div>

<!-- PASTE HTML HERE-->
<h1 id="module:-information-theory--1-credit-/-15-hours">Module: Information Theory – 1 Credit / 15 hours</h1>
<p><strong>Competency: Mathematical Reasoning of Data</strong></p>
<p>Author: Jack Pope</p>
<h2 id="instructor-overview-&amp;-objective">Instructor overview &amp; objective</h2>
<p>This course module is about Information Theory.</p>
<p>Given that this module is to be worth 1 credit, or 15 hours of coursework, the instructor should prepare three to five subtopic discussions. These should account for at least 5 hours of instructional material and might be in the form of lecture notes or lecture videos. The additional hours should be comprised of student labs or assignments.</p>
<p>Traditionally, a course in Information Theory takes a theoretical approach, on paper. However, the field of Data Science is an empirical endeavor that programmatically emphasizes empirical evidence.</p>
<p>Axioms which we accept as true are what support logical proof. Evidence (data) supports empirical proof, certainty based on a measurement and its statistical significance. A weakness of logical proof occurs when erroneous accepted “facts” support an axiom. A weakness of the empirical proof occurs when the data sample does not represent a population.</p>
<p>To accommodate our purposes, we will use the Python programming language. The choice of programming language is ultimately left to the instructor.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<table>
<thead>
<tr class="header">
<th>Unit</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Overview of Information Theory</td>
</tr>
<tr class="even">
<td>2</td>
<td>Entropy</td>
</tr>
<tr class="odd">
<td></td>
<td>Exercise</td>
</tr>
<tr class="even">
<td>3</td>
<td>Transmission of messages</td>
</tr>
<tr class="odd">
<td></td>
<td>Joint entropy</td>
</tr>
<tr class="even">
<td></td>
<td>Exercise</td>
</tr>
<tr class="odd">
<td></td>
<td>Conditional entropy</td>
</tr>
<tr class="even">
<td></td>
<td>Exercise</td>
</tr>
<tr class="odd">
<td></td>
<td>Mutual information</td>
</tr>
<tr class="even">
<td></td>
<td>Exercise</td>
</tr>
<tr class="odd">
<td>4</td>
<td>Noise, Error Detection, and Correction</td>
</tr>
<tr class="even">
<td></td>
<td>Exercise</td>
</tr>
<tr class="odd">
<td></td>
<td>Channel capacity</td>
</tr>
<tr class="even">
<td></td>
<td>Exercise</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Data Compression and Huffman Encoding</td>
</tr>
<tr class="even">
<td></td>
<td>Compression</td>
</tr>
<tr class="odd">
<td></td>
<td>Decompression</td>
</tr>
<tr class="even">
<td></td>
<td>Exercise</td>
</tr>
<tr class="odd">
<td>6</td>
<td>Additional References</td>
</tr>
</tbody>
</table>
<h2 id="overview-of-information-theory">Overview of Information Theory</h2>
<p>Information theory, also called Communication Theory, is concerned with the processing and transmission of data. Sometimes Information Theory is referred to as Coding Theory, which is actually more focused on the quantification of data. Related studies include Statistical Signal Processing and Digital Communications (digital-to-analog conversion).</p>
<p>Information theory has three general components: a transmitter, a channel and a receiver. These components apply whether we are communicating over amateur radio or by smoke signal.</p>
<p>In studying Information Theory we are interested in two fundamental limits: the maximum amount of data that can be transmitted over a medium (per second) and the degree of compression that can be achieved without a loss of information.</p>
<p>In the formal context of Information Theory, more information does not equate to clarity, but just the opposite. Information Theory seeks to minimize the amount of data required to convey a message.</p>
<p>“Information: the negative reciprocal value of probability.” – Claude Shannon</p>
<p>A “message” is a series of symbols drawn from a known set. Within a message, a “symbol” can refer to an alphanumeric character, a word, phrase, object, etc. Usually a symbol is represented by a string of 8 bits or one byte.</p>
<p>Each of the symbols used in a communication have a certain quantity relative to the overall message. For example, we typically expect to use the word “the” more often than we use the word “dinosaur.” The word “the” will usually have the higher frequency of use. Usage statistics also apply to characters of the Unicode set, as well as phrases.</p>
<p>Given such expected statistics, methods of data compression can be devised so that fewer bits need to be transmitted. Additionally, symbol statistics will corroborate error correction in messages containing the noise of random information.</p>
<p>Simplistically speaking, given a transmission of eight-bit binary data, there is a frequency for how often a null string such as “00000000” occurs. Lets say that the frequency of the null string is 50% of the data. As a form of compression, to reduce the amount of bits to transmit and store, we can make the null string simply “0,” thereby saving 7 bits. We would then pre-append a 1 in the left-most bit of all the 8-bit strings, making them 9 bits in length. The amount of data saved is then:</p>
<p>Total message bits: 8 x (all null bits + all non-null bits), a multiple of 16 == 128 bits</p>
<p>Total message bits after compression: 8 x [(8 bits – 7 bits) + (8 bits + 1 bit)] == 80 bits</p>
<p>That is a savings of 37.5%, given a transmission of 5 bits per symbol; a savings of 3 bits per byte.</p>
<h2 id="entropy">Entropy</h2>
<p>In the context of Information Theory, what is meant by entropy is really “Shannon information,” which is used synonymously as entropy and uncertainty. (We will not address thermodynamic entropy here.)</p>
<p>In general terms, there is “low entropy” if a message is succinct and high entropy when a message is uncertain. Given that more entropy (more Shannon Information) means less certainty, a measure of entropy can guide data compression, as we will see.</p>
<p>As mentioned, symbol frequencies in a message can be used to reduced the number of bits to transmit a message. This requires that both the sender and the receiver share the same symbols statistics. It could be that frequencies are from a single message, or a type of message, or from a sample of prior messages. Perhaps the sender and receiver agree to use a commonly recognized source of word and letter frequencies (such details may be transmitted in advance of the relatively longer communication). For a source see: <a href="http://www.norvig.com/mayzner.html">http://www.norvig.com/mayzner.html</a> . There are also statistics on successive word pairs, trios, etc. For example the letter 't' usually proceeds the letter 'h' in two letter sequences.</p>
<p>There are 26<sup>26</sup> possible letter sequences and, with 97,565 words in then English dictionary, we might consider five million pairs to analyze. However, many of these combinations are not used in normal non-encrypted communication. High frequency words will include conjunctions and propositions. We are interested in the related nouns and verbs.</p>
<p>Lets summarize some statistics based on what we know so far about entropy and the English language. We will assume independence between successive characters and successive words.</p>
<p>English language quantifications:</p>
<ul>
<li>Characters (A to Z and blank space): 27</li>
<li>Words: 97,565</li>
<li>Average word length: 4.79 chars</li>
<li>Uniform character distribution: log<sub>2</sub>(27) == 4.75 bits</li>
<li>Uniform word distribution (incl space): log<sub>2</sub>(97,565) / 5.79 == 2.86 bits</li>
</ul>
<p>Consider the characters of the English alphabet, with 26 letters and a blank space (a hidden \s character) for separating words. So, we really have 27 characters.</p>
<p>Given that the probability distribution of a message is known, the length of the message can be known. Therefore, for all bits of message S, the sender and receiver have the same information, or decode(encode(S<sub>i</sub>)) = S<sub>i</sub></p>
<p>The computed entropy depends on the basis of symbol probability that is adopted by the sender and receiver.</p>
<p>Minimum bits to represent a symbol:</p>
<p>The essence of Shannon's 1948 “Noiseless Coding Theorem” is that no redundant data be used in a message. Accordingly, the theoretical minimum length of a message is given by the entropy of the message.</p>
<p>The expected length of message is equal to the entropy of the message.</p>
<p>Entropy, also known as Shannon Entropy or Shannon Information, can be expressed as:</p>
<p>I = -log<sub>2</sub> P<sub>0</sub></p>
<p>where P<sub>0</sub> is the probability that a message will be received.</p>
<p>Single event:</p>
<p>The Shannon Information I may be expressed for a single event e as inversely related to the probability of the event. That is:</p>
<p>I<sub>e </sub>= -log<sub>2</sub> P<sub>e </sub>== log<sub>2</sub> (1/ P<sub>e</sub>)</p>
<p>You can use either form.</p>
<p>Thus, with 100% certainty,</p>
<p>I = -log<sub>2</sub> 1 = 0</p>
<p>So, 0 bits of “Shannon information” (Same as 0 bits of Shannon entropy).</p>
<p>Note that logarithms in base 2 count in magnitudes of 2, readily accommodating bits (binary digits). If the values that we are dealing with are in another base, we can still reflect these as bits by computing the amount of information as the log_baseX(someNumber) / log_baseX(2).</p>
<p>Working with distinct 8-bit string lengths, from 00000000 to 11111111, the receiver will not confuse 00000111 with 1110. Binary can also corresponds with some decision trees and networking protocols.</p>
<p>Apart from reflecting binary magnitudes, using a logarithm relegates output to a narrower range of integers.</p>
<p>Consider the decimal value 0.5 in terms of bits:</p>
<p>I = -log(0.5)/log(2) == -log<sub>2</sub> 0.5 == 1 bit</p>
<p>Supposing that a message has a 0.5 probability of being sent (like flipping a coin), the receiver cannot be certain which message will be received.</p>
<p>Lets consider a scenario where the transmitter can send any one of 10 possible, equally probable, messages.</p>
<p>I = -log<sub>2</sub> (1/10) = 3.3</p>
<p>This is 3.3 bits of Shannon information or Shannon entropy.</p>
<p>If it is known that the symbols 'B' occurs 8 times in 64 symbols, then the number of bits required to represent 'B” is:</p>
<p>-log<sub>2</sub> 8/64 == 3 bits</p>
<p>In the above example, the expected length is 3, which is also its entropy. Using more than 3 bits to represent 'B' suggests opportunity for compression.</p>
<p>Weighted average entropy:</p>
<p>We can define entropy as the average uncertain information received over a range of values or events.</p>
<p>For a set of symbols comprising a message, weighted average entropy may be denoted by the following equation:</p>
<p>H = -∑ P<sub>i</sub> log<sub>2</sub> P<sub>i</sub></p>
<p>That is, P<sub>i</sub> is the probability of a symbol. While all symbol frequency probabilities will sum to 100%, entropy may be computed higher and lower than 1. Entropy of 0 indicates certainty.</p>
<p>Note the probabilities now also serve as weights, as now needed to give relative weight to each parameter:</p>
<p>I = -(P<sub>0 </sub>x log<sub>2</sub> P<sub>0 </sub>+ P<sub>1 </sub>x log<sub>2</sub> P<sub>1</sub>)</p>
<p>Given a set of two symbols, with probabilities of 0.60 and 0.40, the entropy is 0.971, indicating entropy or uncertainty. In a spreadsheet, the formula can be entered as:</p>
<p>=-(0.6*LOG(0.6,2)+0.4*LOG(0.4,2))</p>
<p>It would be the same result if 4 of 10 messages share had one expectation while the other 6 shared another expectation of being received. Suppose the message was simply “AAAABBBBBB.” The string is four tenths As and six tenths Bs. The formula is like that in the above scenario:</p>
<p>I = -(0.40 x log<sub>2</sub> 0.40 + 0.6 x log<sub>2</sub> 0.6) == 0.971 bits of Shannon information</p>
<p>Dealing with unknowns:</p>
<p>We can limit uncertainty by imposing a maximum bound of likelihood of receipt for a set of symbols or messages. We may apply such a bound to a range of symbols within a set of unknown or random information, treating all equally likely. That way, we get results for groups of symbols, like the strings “AAAA” and “BBBBBB” in the example above. We can similarly have categories of symbols with various weights. Symbols of unknown probability can be assigned 0.5 as a neutral weight.</p>
<p>Consider for example that 9 out of 10 messages are equally expected by the receiver. It could be that a “constraint” was imposed on this set of 9. Therefore, there is only a 10% expectation of receiving the one unique message. Now the Shannon entropy is:</p>
<p>I = -(0.90 x log<sub>2</sub> 0.90 + 0.10 x log<sub>2</sub> 0.10) = 0.20</p>
<p>This means that there are 0.20 bits of Shannon entropy or uncertainty of 0.20 bits. The constraint effectively limited the amount of information, thereby reducing uncertainty.</p>
<p>If we have only a small set of symbols, the number list [1, 3, 5, 7]. The entropy calculation is:</p>
  <blockquote>
  Element, e from index 1 to 4<br>
  Number of elements, N = 4  <br>
  Probability of e, P<sub>e</sub> = P<sub>e</sub> / N <br>  
  Shannon entropy, I = -1 \* (average sum of P<sub>e</sub> \* log<sub>2</sub>(Pi) ) == 2 bits
  </blockquote>

<p>What if the the sample numbers are [2, 4, 6, 8], the result is the same, 2 bits. If you are confused, think though this general algorithm (program code is AWK):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode a"><code class="sourceCode ada"><a class="sourceLine" id="cb1-1" title="1">bitstr = <span class="st">&quot;1111222233334444&quot;</span></a>
<a class="sourceLine" id="cb1-2" title="2">N = split(bitstr, arr, <span class="st">&quot;&quot;</span>)      # create <span class="kw">array</span> arr N = length <span class="kw">of</span> bitstr</a>
<a class="sourceLine" id="cb1-3" title="3"></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="kw">for</span> (i = <span class="dv">1</span>; i &lt;= N; i++) {      # assign value <span class="kw">of</span> digit to a key <span class="kw">in</span> <span class="kw">array</span> arr2</a>
<a class="sourceLine" id="cb1-5" title="5">  arr2[arr[i]]++;               # Note! Duplicates == single key; yet +<span class="dv">1</span> <span class="kw">for</span> each digit</a>
<a class="sourceLine" id="cb1-6" title="6">}</a>
<a class="sourceLine" id="cb1-7" title="7"></a>
<a class="sourceLine" id="cb1-8" title="8"><span class="kw">for</span>(e <span class="kw">in</span> arr2) {</a>
<a class="sourceLine" id="cb1-9" title="9">  # now we want count <span class="kw">of</span> element relative to total element count</a>
<a class="sourceLine" id="cb1-10" title="10">  p = arr2[e] / N;     # key e <span class="kw">for</span> val = relative count <span class="kw">in</span> numerator / total count </a>
<a class="sourceLine" id="cb1-11" title="11">  I -= p * log(p);</a>
<a class="sourceLine" id="cb1-12" title="12">}</a>
<a class="sourceLine" id="cb1-13" title="13">print I / log(<span class="dv">2</span>);</a></code></pre></div>
<p>In Python the same algorithm can look like this:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="co">#!/usr/bin/python3 </span></a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">import</span> math</a>
<a class="sourceLine" id="cb2-3" title="3"><span class="kw">def</span> entropy(bitstr):</a>
<a class="sourceLine" id="cb2-4" title="4">  elist <span class="op">=</span> <span class="bu">list</span>(bitstr)</a>
<a class="sourceLine" id="cb2-5" title="5">  N <span class="op">=</span> <span class="bu">len</span>(elist)</a>
<a class="sourceLine" id="cb2-6" title="6">  I <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb2-7" title="7">  S <span class="op">=</span> <span class="bu">set</span>(elist) <span class="co"># creates set of distinct elements</span></a>
<a class="sourceLine" id="cb2-8" title="8"></a>
<a class="sourceLine" id="cb2-9" title="9">  <span class="cf">for</span> s <span class="kw">in</span> S:</a>
<a class="sourceLine" id="cb2-10" title="10">    frequency <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb2-11" title="11">    <span class="cf">for</span> e <span class="kw">in</span> elist:</a>
<a class="sourceLine" id="cb2-12" title="12">      <span class="cf">if</span>(s <span class="op">==</span> e):</a>
<a class="sourceLine" id="cb2-13" title="13">        frequency <span class="op">=</span> frequency <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb2-14" title="14">        p <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">*</span> frequency) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">*</span> N)</a>
<a class="sourceLine" id="cb2-15" title="15">      I <span class="op">=</span> I <span class="op">-</span> p <span class="op">*</span> math.log(p)</a>
<a class="sourceLine" id="cb2-16" title="16"></a>
<a class="sourceLine" id="cb2-17" title="17">  <span class="cf">return</span> I <span class="op">/</span> math.log(<span class="dv">2</span>) </a>
<a class="sourceLine" id="cb2-18" title="18"></a>
<a class="sourceLine" id="cb2-19" title="19"><span class="co">#Run a test:</span></a>
<a class="sourceLine" id="cb2-20" title="20">bitstr <span class="op">=</span> <span class="st">&quot;1111222233334444&quot;</span></a>
<a class="sourceLine" id="cb2-21" title="21"><span class="bu">print</span>(entropy(bitstr))</a></code></pre></div>
<p>Whatever language you use, when computing P<sub>e</sub> be careful not to confuse the count of distinct elements with the elements themselves. If you use an associative array to assist with this task, the manner of extracting a collection's keys and values depends on the language.</p>
<h2 id="transmission-of-messages">Transmission of messages</h2>
<p>Our message symbols can be comprised of characters or words.</p>
<p>The Shannon information for drawing a specific character, such as 'A':</p>
<p>I = -log<sub>2</sub> (1/27) = 4.76</p>
<p>The Shannon information for drawing the three character sequence 'ABC':</p>
<p>I = -log<sub>2</sub> (1/27) + -log<sub>2</sub> (1/27) + -log<sub>2</sub> (1/27) == 3 x ( -log<sub>2</sub> (1/27) ) = 14.28</p>
<p>That same value of Shannon information exists if we seek 'XYZ' or 'ABC' or whether the characters were received without error or random.</p>
<p>Using large volume for symbol frequency counts:</p>
<p>In reality, a message will have some characters that are more frequent than other characters. Based on the statistics shown earlier, a blank space typically occupies 20% of a word symbol. E is the most used letter, with 12.49% of the total letter count, and Z is the least used letter, with 0.09% usage (Source: <a href="http://www.norvig.com/mayzner.html">http://www.norvig.com/mayzner.html</a> ).</p>
<p>Computing Shannon Information can take these statistics into account. For example, different parameters of the weighted entropy formula can represent each character with a different weight based on its probability. That is:</p>
<p>I = -(P<sub>blank</sub> x log<sub>2</sub> P<sub>blank</sub> + P<sub>A</sub> x log<sub>2</sub> P<sub>A</sub> + P<sub>B</sub> x log<sub>2</sub> P<sub>B</sub> + … + P<sub>Z</sub> x log<sub>2</sub> P<sub>Z</sub> )</p>
<p>Based on the relative parameters of this weighted average, the <em>expectation of drawing any one character</em>, the Shannon information, is 4.16 bits.</p>
<p>Supposing communication of three specific characters, in any order, lets consider the probability of their being drawn. For the characters “HOP” we have:</p>
<p>I = -(P<sub>H</sub> x log<sub>2</sub> P<sub>H</sub> + P<sub>O</sub> x log<sub>2</sub> P<sub>O</sub> + P<sub>P</sub> x log<sub>2</sub> P<sub>P</sub> )</p>
<p>Or,</p>
<p>I = -(0.05 x log<sub>2</sub> 0.05 + 0.076 x log<sub>2</sub> 0.076 + 0.02 x log<sub>2</sub> 0.02 ) = 0.62 bits</p>
<p>The likelihood of receiving a message “HOP” is not different than receiving “POH.”</p>
<p>In Information Theory we are interested in Shannon entropy given a set of symbols in any order.</p>
<p>If we require exact order, such as H-O-P, then we can look to Probability Theory. As an aside, consider that each character H-O-P has its own probability of occurrence. To receive these characters in that exact sequence is less probable than any one of their probabilities. That overall probability would be:</p>
<p>0.05 x 0.076 x 0.02 == 0.000076</p>
<p>Based on that result, one might compute the Shannon entropy of a single draw of these three characters as:</p>
<p>I = -log<sub>2</sub> 0.000076 = 13.68</p>
<p>… indicating a high amount of entropy or uncertainty for receiving the exact symbol sequence H-O-P.</p>
<p>However, for Information Theory, whether a symbol sequence is recognizable or random is irrelevant. What matters is the weighted average number of bits. “HOP” has the same information as “POH.”</p>
<p>The meaning of information is of no concern to Information Theory. In that context, “more information” does not convey more meaning, but greater uncertainty with a greater number of bits that must be processed.</p>
<p>Exercise</p>
<p>Using Python, write an algorithm to determine the entropy of the following lists:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</a>
<a class="sourceLine" id="cb3-2" title="2">[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb3-3" title="3">[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]</a></code></pre></div>
<p>Submit your coded algorithm and your output.</p>
<p>What do you deduce about the impact of various element patterns on entropy?</p>
<p>Joint entropy</p>
<p>Given two sets of symbols, joint entropy is the entropy of their intersecting set. That is, it is the count of each element within the intersection relative to the total number of intersecting elements.</p>
<p>Formally, joint entropy can be expressed as:</p>
<p>H(S1 , S2) = -∑<sub>i</sub>P(S1<sub>i </sub>, S2<sub>j</sub>) x log<sub>2</sub>(P(S1<sub>i</sub> , S2<sub>j</sub>)), where (S1<sub>i </sub>, S2<sub>j </sub>) ∈ S1 × S2</p>
<p>Determining joint entropy is not much different than finding entropy, except that you must first determine the intersecting set.</p>
<p>Python gives us some fairly easy to use set operators:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">s1, s2 <span class="op">=</span> {<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">8</span>}, {<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>}   <span class="co"># define 2 sets</span></a>
<a class="sourceLine" id="cb4-2" title="2"><span class="bu">print</span>(<span class="st">&quot;union s1 s2: &quot;</span>, s2 <span class="op">|</span> s2)</a>
<a class="sourceLine" id="cb4-3" title="3"><span class="bu">print</span>(<span class="st">&quot;intersection s1 s2: &quot;</span>, s1 <span class="op">&amp;</span> s2)</a>
<a class="sourceLine" id="cb4-4" title="4"><span class="bu">print</span>(<span class="st">&quot;difference s1 s2: &quot;</span>, s1 <span class="op">-</span> s2)</a></code></pre></div>
<p>However, for you to better understand the set operations involved, the following Python code is more explicit about the set intersection:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="co">#!/usr/bin/python3</span></a>
<a class="sourceLine" id="cb5-2" title="2"><span class="im">import</span> math</a>
<a class="sourceLine" id="cb5-3" title="3"><span class="kw">def</span> jointEntopy(bitstr1, bitstr2):</a>
<a class="sourceLine" id="cb5-4" title="4">  S1 <span class="op">=</span> <span class="bu">set</span>(bitstr1)</a>
<a class="sourceLine" id="cb5-5" title="5">  S2 <span class="op">=</span> <span class="bu">set</span>(bitstr2)</a>
<a class="sourceLine" id="cb5-6" title="6">  L1 <span class="op">=</span> <span class="bu">list</span>(bitstr1)</a>
<a class="sourceLine" id="cb5-7" title="7">  L2 <span class="op">=</span> <span class="bu">list</span>(bitstr2)</a>
<a class="sourceLine" id="cb5-8" title="8">  N <span class="op">=</span> <span class="bu">len</span>(L1)</a>
<a class="sourceLine" id="cb5-9" title="9">  I <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb5-10" title="10">  <span class="cf">for</span> s1 <span class="kw">in</span> S1:</a>
<a class="sourceLine" id="cb5-11" title="11">    <span class="cf">for</span> s2 <span class="kw">in</span> S2:</a>
<a class="sourceLine" id="cb5-12" title="12">      frequency <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb5-13" title="13">      <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb5-14" title="14">        <span class="cf">if</span>(s1 <span class="op">==</span> L1[i] <span class="kw">and</span> s2 <span class="op">==</span> L2[i]):     <span class="co"># set intersection </span></a>
<a class="sourceLine" id="cb5-15" title="15">          frequency <span class="op">=</span> frequency <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb5-16" title="16">      p <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">*</span> frequency) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">*</span> N)</a>
<a class="sourceLine" id="cb5-17" title="17">      <span class="cf">if</span> p <span class="op">&gt;</span> <span class="fl">0.0</span>:</a>
<a class="sourceLine" id="cb5-18" title="18">        I <span class="op">=</span> I <span class="op">-</span> p <span class="op">*</span> math.log(p)</a>
<a class="sourceLine" id="cb5-19" title="19"></a>
<a class="sourceLine" id="cb5-20" title="20">  <span class="cf">return</span> ( I <span class="op">/</span> math.log(<span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb5-21" title="21"></a>
<a class="sourceLine" id="cb5-22" title="22"><span class="co">#Run a test:</span></a>
<a class="sourceLine" id="cb5-23" title="23">bitstr1 <span class="op">=</span> <span class="st">&quot;13578&quot;</span></a>
<a class="sourceLine" id="cb5-24" title="24">bitstr2 <span class="op">=</span> <span class="st">&quot;12468&quot;</span></a>
<a class="sourceLine" id="cb5-25" title="25"><span class="bu">print</span>( jointEntopy(bitstr1, bitstr2) )</a></code></pre></div>
<p>Exercise</p>
<p>Using Python, write an algorithm to determine the joint entropy of the following lists:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">        S1                     S2</a>
<a class="sourceLine" id="cb6-2" title="2">[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>] <span class="kw">and</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb6-3" title="3">[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>] <span class="kw">and</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>] </a>
<a class="sourceLine" id="cb6-4" title="4">[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>] <span class="kw">and</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>]  </a></code></pre></div>
<p>Submit your coded algorithm and your output.</p>
<p>What do you deduce about the impact of any particular bit pattern on joint entropy?</p>
<p>What types of patterns increase or decrease joint entropy?</p>
<p>Conditional entropy</p>
<p>There are cases in which one variable depends on another. Coincident signals may be depicted as cause-affect relationships.</p>
<p>The entropy of S2 conditioned on S1 is written as: H(S1 | S2). More formally: If H ( S2 | S1 = s1 ) is the entropy of the discrete random variable S2 conditioned on the discrete random variable S2 taking a certain value s1, then H ( S2 | S1 ) is the result of averaging H ( S2 | S1 = s1 ) over all possible values s1 that S2 may take.</p>
<p>Formally, conditional entropy can be expressed as:</p>
<p>H(S1 , S2) = -∑<sub>i</sub>∑<sub>j</sub>P(S1<sub>i </sub>, S2<sub>j</sub>) x log<sub>2</sub>(P(S1<sub>i</sub> , S2<sub>j</sub>)), where S1<sub>i </sub>∈ S1 and S2<sub>j </sub> ∈ S2</p>
<p>We can account for their joint entropy, comparing it to the entropy of the other, perhaps dependent, set. That is:</p>
<p>H(S1 | S2) = jointEntropy(S1, S2) - entropy(S2)</p>
<p>Presented with dependence on another variable we have a state of ambiguity. That is why conditional entropy is also called “equivocation.” In the formula above, S1 may be interpreted as a “cause” while S2 is considered an “effect.” H(S1 | S2) is the signal lost between S1 and S2.</p>
<p>Cause and effect exist when the sender can confirm the receiver's information, or if the receiver can confirm what the sender actually sent. Given uncertainty, where S2 is not likely to reflect a known S1, there might be set independence or channel noise (see below).</p>
<p>In Python, using our algorithms above for entropy and joint entropy, conditional entropy may be described as follows:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="co">#!/usr/bin/python3</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="bu">exec</span>(<span class="bu">open</span>(<span class="st">&quot;entropy.py&quot;</span>).read())</a>
<a class="sourceLine" id="cb7-3" title="3"><span class="bu">exec</span>(<span class="bu">open</span>(<span class="st">&quot;jointEntropy.py&quot;</span>).read())</a>
<a class="sourceLine" id="cb7-4" title="4"><span class="kw">def</span> conditionalEntropy(bitstr1, bitstr2):</a>
<a class="sourceLine" id="cb7-5" title="5">  <span class="cf">return</span> jointEntropy(bitstr1, bitstr2) <span class="op">-</span> entropy(bitstr2) </a>
<a class="sourceLine" id="cb7-6" title="6"></a>
<a class="sourceLine" id="cb7-7" title="7"><span class="co"># Run a test:</span></a>
<a class="sourceLine" id="cb7-8" title="8">bitstr1 <span class="op">=</span> <span class="st">&quot;13578765&quot;</span></a>
<a class="sourceLine" id="cb7-9" title="9">bitstr2 <span class="op">=</span> <span class="st">&quot;12468765&quot;</span></a>
<a class="sourceLine" id="cb7-10" title="10"><span class="bu">print</span>( conditionalEntropy(bitstr1, bitstr2) )</a></code></pre></div>
<p>Exercise</p>
<p>Using Python, write an algorithm to determine the conditional entropy of the following lists:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1">        S1                     S2</a>
<a class="sourceLine" id="cb8-2" title="2">[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>] <span class="kw">and</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb8-3" title="3">[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>] <span class="kw">and</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>] </a>
<a class="sourceLine" id="cb8-4" title="4">[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>] <span class="kw">and</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>]  </a></code></pre></div>
<p>Submit your coded algorithm and your output.</p>
<p>What do you deduce about the impact of any particular bit pattern on conditional entropy?</p>
<p>What types of patterns increase or decrease conditional entropy?</p>
<p>Mutual information</p>
<p>Mutual information can tell us how much uncertainty is removed from one message given (prior) knowledge of another message.</p>
<p>Consider the following formula for mutual information:</p>
<blockquote>
H(S1 | S2) = entropy(S1) - conditionalEntropy(S1, S2) <br>
 &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp= entropy(S1) - jointEntropy(S1, S2) - entropy(S2)
</blockquote>

<p>The mutual information of two sets of symbols gauges in terms of Shannon Information how much dependence set S1 has on S2 and how much dependence set S2 has on S1.</p>
<p>In Python, mutual information might look like this:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="co">#!/usr/bin/python3</span></a>
<a class="sourceLine" id="cb9-2" title="2"><span class="bu">exec</span>(<span class="bu">open</span>(<span class="st">&quot;entropy.py&quot;</span>).read())</a>
<a class="sourceLine" id="cb9-3" title="3"><span class="bu">exec</span>(<span class="bu">open</span>(<span class="st">&quot;conditionalEntropy.py&quot;</span>).read())</a>
<a class="sourceLine" id="cb9-4" title="4"></a>
<a class="sourceLine" id="cb9-5" title="5"><span class="kw">def</span> mutualInformation(bitstr1, bitstr2):</a>
<a class="sourceLine" id="cb9-6" title="6">  <span class="cf">return</span> entropy(bitstr1) <span class="op">-</span> conditionalEntropy(bitstr1, bitstr2)          </a>
<a class="sourceLine" id="cb9-7" title="7"></a>
<a class="sourceLine" id="cb9-8" title="8"><span class="co"># Run a test:</span></a>
<a class="sourceLine" id="cb9-9" title="9">bitstr1 <span class="op">=</span> <span class="st">&quot;13578765&quot;</span></a>
<a class="sourceLine" id="cb9-10" title="10">bitstr2 <span class="op">=</span> <span class="st">&quot;12468765&quot;</span></a>
<a class="sourceLine" id="cb9-11" title="11"><span class="bu">print</span>(mutualInformation(bitstr1, bitstr2))</a></code></pre></div>
<p>Exercise</p>
<p>Using Python, write an algorithm to compute the mutual information for any two sets, S1 and S2. Show what happens when there is redundant data encoding for overcoming symbol errors. What is the implication for redundancy given mutual information?</p>
<p>Submit your coded algorithm and your output.</p>
<h2 id="noise,-error-detection,-and-correction">Noise, Error Detection, and Correction</h2>
<p>Noise amounts to uncertainty. It adds more information, increasing entropy.</p>
<p>Simplistic approaches to dealing with error include sending redundant messages or symbols and using a parity bit.</p>
<p>Ignoring channel capacity, redundant transmission maintains the same probability of drawing any one character from a message.</p>
<p>A message can be sent twice and the two instances of received message can be compare. The information that these messages have in common provides some accepted degree of validation. For example, if there is no error, if some information corroborates that received symbols are correct, the mutual information will be equivalent to the joint entropy of the redundant sets.</p>
<p>Recall that for mutual information, H(S1 | S2) = entropy(S1) - conditionalEntropy(S1, S2). Through substitution, we have H(S1 | S2) = entropy(S1) - jointEntropy(S1, S2) – entropy(S2). If S1== S2, then H(S1 | S2) = jointEntropy(S1, S2).</p>
<p>To reduce the amount of redundant data for overcoming errors, there is another approach: use a “parity bit.” That is to send one bit between each message that indicates an odd (0) or even (1) number of bits. However, this is only good for single bit errors.</p>
<p>The parity of a list can be computed as:</p>
<p>parity = sum(S<sub>i</sub>) % 2</p>
<p>The result can be compared to the received parity bit.</p>
<p>Redundant characters can be transmitted. For example, instead of transmitting “HOP” transmit characters as double characters, such as “HHOOPP.”</p>
<p>The receipt of “HHOXPP” clearly has an error. This can trigger a “re-transmit” request, or the receiver may simply substitute the most common word having the pattern “H_P.”</p>
<p>A more robust scheme is transmit several instances of each character:</p>
<pre><code>H H H H H H H H
O O O O O O O O
P P P P P P P P
</code></pre>
<p>Each character actually has its ASCII code represented in binary, as 0 and 1 bits. The representation of a 1 being in the aforementioned redundant 8 bit scheme. These redundant bits provide “7 bit error correction.” For example, a single 1 bit is expressed as:</p>
<pre><code>1 1 1 1 1 1 1 
</code></pre>
<p>… and a single 0 bit is expressed as:</p>
<pre><code>0 0 0 0 0 0 0 0
</code></pre>
<p>Following each of these bit strings with parity bit, such as 00000000 or 00000001, provides “n-bit error detection.”</p>
<p>The receiver can decode the value S2 = 11111111 as 1 if the sum(S1) &gt; 4 or if the subsequent parity bit is 0.</p>
<p>Additionally, there could be a certain parity for certain vertical sequences of bit strings for “m-bit error detection.”</p>
<p>To a certain extent, the transmitter and receiver must share some a priori information, if not about redundancies and parities, at least about basic communication protocols, such as encoding and modulations. Otherwise, the there will be no receipt of signal, just noise. The medium does not need to know this information.</p>
<p>Exercise</p>
<p>Code an algorithm that demonstrates use of redundant symbols for overcoming errors. Test your algorithm and show its output. Does symbol redundancy change affect Shannon entropy? Explain why or why not.</p>
<h3 id="channel-capacity">Channel capacity</h3>
<p>The medium or route connecting a transmitter to a receiver may be called a channel. The channel may carry electrical pulses, representing bits according to a protocol, such as for networking or wireless communication. Bits may represent integers, and integers may represent characters which comprise words of communication.</p>
<p>A channel is limited by how many bits it can carry without signal loss. Suppose a electrically conductive medium maintains eight voltage fluctuations per second (a frequency of 8 Hertz), each with a consistent magnitude. The most data that could be effectively transmitted on this channel is 8 bits/s. The channel's bandwidth is 8 bits/s.</p>
<p>Regardless of the communication protocol, an increase in the rate of throughput will require either a higher rate of voltage fluctuation per second (higher frequency) or transmission on multiple channels in parallel. In either case, a higher rate of transmission requires more energy.</p>
<p>If errors received increase at an increasing rate as the rate of symbol throughput increases, then the probability of reception should be reduced until entropy resides within an acceptable tolerance for the error correction scheme in place.</p>
<p>A communication channel's capacity is the maximum rate for transmitting symbols in bits per second. That is:</p>
<p>Given the transmission of one message per cycle, information rate is defined as:</p>
<p>informationRate(S1, S2) = channelCapacity(S1, S2)</p>
<p>Accordingly, Shannon's Fundamental Theorem for a Noiseless Channel (1948) holds that there is a maximum rate S for symbol transmission based on channel capacity C divided by the entropy H:</p>
<p>S = C / H</p>
<p>We can consider channel capacity the maximum mutual information and compute it as:</p>
<p>channelCapacity(S1, S2) = mutualInformation(S1, S)</p>
<h3 id="exercise">Exercise</h3>
<p>Using Python, write an algorithm to determine the channel capacity of the following lists:</p>
<pre><code>        S1                     S2
[0, 0, 0, 0, 0, 0] and [1, 1, 1, 1, 1, 1]
[1, 0, 1, 0, 1, 0] and [0, 1, 0, 1, 0, 1] 
[1, 2, 2, 3, 3, 3] and [1, 1, 1, 2, 2, 3]  
</code></pre>
<p>What values of 0s and 1s for S1 and S2 will maximize channel capacity? Does information rate matter?</p>
<h2 id="data-compression-and-huffman-encoding">Data Compression and Huffman Encoding</h2>
<p>Data compression is an achievement of information theory. Compressing data allows us to represent information with fewer bits for transmission and storage. The receiver restores the original information using decompression.</p>
<p>Shannon entropy makes compression possible, as it tells us when data is represented with more bits than necessary. By representing a symbol from a message with an optimal number of bits to minimize entropy, the bit sequence representing a message will have no more bits than is required for its unique identity.</p>
<p>Depending on the compression method, more bits can be acceptably compressed than can be restored through decompression. Such “lossy” data compression maybe acceptable for sound and graphics processing. Conversely, archival storage of financial data must be “lossless.” Similarly, data transfer over a network may be either lossless or lossy, depending on the application.</p>
<p>The effectiveness of compression is the size of the compressed data relative to the original data.</p>
<p>We will look at lossless compression in terms of Huffman encoding, a “minimum redundancy coding” which encodes the most frequently used symbols by using the fewest bits. There are other clever techniques that deserve a place in a dedicated course of study on data compression, such as Dynamic Markov compression and LZ77 compression.</p>
<h3 id="compression">Compression</h3>
<p>Huffman coding is an elegant form of compression that decodes and encodes data using a binary tree data structure called a “Huffman tree.”</p>
<p>As an example, suppose we have a text message to send that consists of 120 characters, only made up of the characters A, B, C, X, Y, and Z. These characters have different frequencies of occurrence in the text. So, we itemize their probabilities and entropies as follows:</p>
<table>
<thead>
<tr class="header">
<th>Symbol</th>
<th>Probability</th>
<th>Entropy ea</th>
<th>Entropy total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>27 / 120</td>
<td>0.48</td>
<td>13.07</td>
</tr>
<tr class="even">
<td>B</td>
<td>9 / 120</td>
<td>0.28</td>
<td>2.52</td>
</tr>
<tr class="odd">
<td>C</td>
<td>21 / 120</td>
<td>0.44</td>
<td>9.24</td>
</tr>
<tr class="even">
<td>X</td>
<td>15 / 120</td>
<td>0.38</td>
<td>5.63</td>
</tr>
<tr class="odd">
<td>Y</td>
<td>12 / 120</td>
<td>0.33</td>
<td>3.99</td>
</tr>
<tr class="even">
<td>Z</td>
<td>16 / 120</td>
<td>0.39</td>
<td>6.20</td>
</tr>
</tbody>
</table>
<p>Building Huffman tree codes result in compression that approximates entropy. It cannot be exact because, as you can see, entropy does not compute to whole numbers.</p>
<p>To compress and decompress, start with one of the symbols, such as 'A.' That will signify the root node of the tree. A left branch from a node will be labeled with a 0 and a right branch will be labeled with a 1.</p>
<p>Order the symbols in terms of frequency:</p>
<pre><code>B  Y  X  Z  C  A
9 12 15 16 21 27
</code></pre>
<p>Sum the symbols pair-wise, pairing the symbols with the smallest probabilities, to create sub trees with their respective root nodes as their sums:</p>
<pre><code>  21        31          48
 /  \      /  \        /  \
B    Y    X    Z      C    A
9    12   15   16    21    27
</code></pre>
<p>Continue to merge the trees, paring the smallest sums:</p>
<pre><code>       52
    /      \
  21        31          48
 /  \      /  \        /  \
B    Y    X    Z      C    A
9    12   15   16    21    27
</code></pre>
<p>Repeat until there is one root. Then label 0s for left branches and 1s for right branches, as follows:</p>
<pre><code>            100
         /       \  
       0/         \1
       /           \
       52           48
   0/     \1      0/  \1
  21       31     C    A 
0/  \1   0/   \1  21   27 
B    Y   X     Z      
9    12  15    16 
</code></pre>
<p>The Huffman code (the compression) for each symbol is found by tracing its path from the root node (top) to the symbol. Along this path make a list of the 0s and 1s. For the above tree, we get:</p>
<pre><code>B = 000
Y = 001
X = 010
Z = 011
C = 10
A = 11
</code></pre>
<p>How much compression did this give us? It takes just 2 to 3 bits to represent all the symbols. If the original scheme were to transmit 8 bit symbols, the Huffman encoding, saves 5 to 6 bits per symbol.</p>
<table>
<thead>
<tr class="header">
<th>Symbol</th>
<th>Frequency</th>
<th>Bits</th>
<th>Frequency x Bits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>27</td>
<td>2</td>
<td>54</td>
</tr>
<tr class="even">
<td>B</td>
<td>9</td>
<td>2</td>
<td>18</td>
</tr>
<tr class="odd">
<td>C</td>
<td>21</td>
<td>3</td>
<td>66</td>
</tr>
<tr class="even">
<td>X</td>
<td>15</td>
<td>3</td>
<td>45</td>
</tr>
<tr class="odd">
<td>Y</td>
<td>12</td>
<td>3</td>
<td>36</td>
</tr>
<tr class="even">
<td>Z</td>
<td>16</td>
<td>3</td>
<td>48</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td>267 total bits</td>
</tr>
</tbody>
</table>
<p>Saved bits = (120 x 8) – 267 / (120 x 8) == 72.2%</p>
<h3 id="decompression">Decompression</h3>
<p>Decompressing entails rebuilding the Huffman tree. Consider the following Huffman codes and their associated values:</p>
<p>{ B:000, Y:001, X:010, Z:011, C:10, A:11 }</p>
<p>We can determine the placement of each symbol. Starting at the root node, the route to a symbol depends on the left-right direction signified by 0 or 1 respectively. Label the end of each such bit path with the associated symbol. Repeat this processes for all of the symbols, starting from the same root node.</p>
<p>In decompression, the sequence order of symbols:bits does not matter because each Huffman code is unambiguous, regardless of the bits in its parent node (ie: Huffman codes are prefix <em>free</em>).</p>
<h3 id="exercise-1">Exercise</h3>
<p>Using Python, write an algorithm to decompress the Huffman codes from our example: A:11, C:10, Z:011, X:010, Y:001, B:000. Does your algorithm verify that symbol sequence matters for decompression? Does it matter for compression? Please explain.</p>
<p>Create an additional function to compute the effectiveness of the Huffman encoding.</p>
<p>Submit your test output along with your code.</p>
<h2 id="additional-references">Additional References</h2>
<p><a href="https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954/ref=sr_1_1?ie=UTF8&amp;qid=1530200954&amp;sr=8-1&amp;keywords=elements+of+information+theory">https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954</a></p>
<p><a href="https://ils.unc.edu/~losee/informationpractice/index.html">https://ils.unc.edu/~losee/informationpractice/index.html</a></p>

</div>
</div>
</body>
</html>