<!DOCTYPE html>
<html>

<head>
  <title>Mathematical Foundations of Data - Set Theory</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
  <meta name="person" content="" />
  <meta name="course" content="" />


  <!--[if lt IE 9]>
	<link href="/shared/metroStyleIE8.css" type="text/css" rel="stylesheet">
  <![endif]-->


  <link href="inc/metroStyle.css" type="text/css" rel="stylesheet">
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.js" type="text/javascript"></script>
  <script src="inc/metroScript.js" type="text/javascript"></script>

  <!--LaTex Includes
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.4.1/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
  <link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">
  -->

  <style>

    .warning {
    visibility: hidden;
  }
  #leftSideBar {
      background-color: #003C66;
      background-repeat: no-repeat;
      background-position: top center;
      background-size: 100%;
      min-height: 100%;
      width: 200px;
      float: left;
      padding-top: 10px;
      padding-right: 1%;
      padding-left: 1%;
      font-size: 1em;
      line-height: 100%;
      color: white;
  }
  
  #leftSideBar a {
      color: white;
      text-decoration: none;
  }
  
  #leftSideBar li {
      margin:0 0 1em 0;
  }
  
  </style>
</head>

<body>
  <p style="text-align: center;"><a name="top"></a></p>
  <div id="pageContainer">
    <div class="toTop"><a href="#top">Top of Page &gt;&gt;</a></div>
    <div id="leftSideBar">
      <img src="images/ITCoE_white200px.png">
      <!---
  <h2 style="color:white;">Sections</h2>
-->
      <hr style="color: white;">
      <ul>
        <li>
          <h2 style="color: white;">Sections</h2>
        </li>
      </ul>
      <hr style="color: white;">
      <ul>
        <!--- Place sidebar links here like this:
    <li><a href="#learning-objectives">Learning Objectives</a></li>
-->
        <li><a href="#overview">1. Overview</a></li>
        <li><a href="#the-achilles-heel-of-regression">2. The Achilles heel of regression</a></li>
        <li><a href="#over-fitting-high-dimensional-data">3. Over-fitting high dimensional data</a></li>
        <li><a href="#detecting-the-curse-of-dimensionality">4. Detecting the curse of dimensionality</a></li>
        <li><a href="#remedying-the-curse-of-dimensionality">5. Remedying the curse of dimensionality</a></li>
        <li><a href="#exercises">6. Exercises</a></li>
        <!--- end sidebar -->
      </ul>
      <div id="printDiv" style="text-align: center;"><input onclick="window.print(); return false;" alt="Print" src="https://metrostate.learn.minnstate.edu/shared/Admin/Templates/Templates/../../../images/icons/print.png"
          width="32" type="image" height="32" /></div>
      <hr style="color: white;">
      <img src="images/mnstate_white200px.png">
    </div>
    <div id="mainContent">
      <div class="warning"><span style="color: #cf2a27;"><strong>Do not delete... If you delete into this warning
            block, Control + Z to Undo... Do not delete</strong></span></div>
<!-- PASTE HTML HERE-->
<h1 id="module:-high-dimensional-data--1-credit-/-15-hours">Module: High Dimensional Data – 1 Credit / 15 hours</h1>
<p><strong>Competency: Big Data</strong></p>
<p>Author: Jack Pope</p>
<p>Note to instructors:</p>
<p><em>Given that the material in this module provides a good introduction and motivation for dimension reduction in data sets, and because the subject matter here may not be enough to merit a 1-credit module, this short module should precede or be merged with the “Dimension Reduction methods” module.</em></p>
<p>Table of Contents</p>
<table>
<thead>
<tr class="header">
<th>Unit</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Overview</td>
</tr>
<tr class="even">
<td>2</td>
<td>The Achilles heel of regression</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Over-fitting high dimensional data</td>
</tr>
<tr class="even">
<td>4</td>
<td>Detecting the curse of dimensionality</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Remedying the curse of dimensionality</td>
</tr>
<tr class="even">
<td>6</td>
<td>Exercises</td>
</tr>
</tbody>
</table>
<h2 id="overview">Overview</h2>
<p>What a waste for the enterprise, to have teams dedicate many hours to developing distributed systems of computation across the enterprise, such as Apache Spark or Hadoop, only to analyze data that is found to be mostly repetitive or noisy. One should better determine in advance the extent of resources that are actually required to analyze the data available.</p>
<p>It may be the case that the researcher is looking for anomalies in abundant data. Preprocessing the data will be time consuming. There may even be a lot of redundant data. Many data fields might contain similar data. Dealing in “big data” makes the problem worse because broadly dispersed unstructured data hinders casual inspection.</p>
<p>To understand the predicaments of high dimensional data, lets for consider some definitions:</p>
<ul>
<li><p>Dimensions: features, fields or predictors.</p></li>
<li><p>Observations: samples, records, feature vectors.</p></li>
<li><p>Feature vector: single observations of a set of features.</p></li>
<li><p>Feature matrix: multiple observations of feature vectors.</p></li>
<li><p>Longitudinal data: records</p></li>
<li><p>Latitudinal data: predictors; features</p></li>
</ul>
<p>Situations that compound the predicament of high dimensional data:</p>
<ul>
<li><p>More sample data is not available and predictors outnumber records.</p></li>
<li><p>Data transformations (linear and non-linear) multiply the number of predictors.</p></li>
<li><p>Lags for time series data multiply the number of predictors.</p></li>
</ul>
<p>Data transformations, whether linear or non-linear, have the potential to multiply the number of predictors. Consider:</p>
<p>predictorCount = predictorCount x transformationCount</p>
<p>If we additionally want to include lags for each time series record and transformation, then our predictor count could be:</p>
<p>predictorCount = predictorCount x transformationCount x lagCount</p>
<p>One way or another we end up cursed by dimensionality. Predictors exceed observations and/or we over-fit the data and/or we have data fields that are redundant.</p>
<h2 id="the-achilles-heel-of-regression">The Achilles heel of regression</h2>
<p>Regression suffers from too many parameters and multicollinearity with highly dimensional data.</p>
<p>We cannot define a regression model if the number or predictors (p) exceed the number of observations (n), as all p cannot be shown with the data available to be linearly independent or to have significantly distinct patterns of data.</p>
<p>A linear regression is considered “well-defined” when its predictor variables are linearly independent. Consider the classical linear regression:</p>
<p>Y = Xβ + ϵ</p>
<p>If (X′X)<sup>−1</sup> must exist for the OLS estimator β = (X′X)<sup>-1</sup> X′Y to be valid, then with p &gt; n we cannot use OLS.</p>
<p>An expedient but less than ideal solution is to test several randomly sampled fields of either the original data set or transformations of the original data. With access to more population of data, we can randomly sample more records and predictors. A better solution is to first reduce the dimensionality of the data set.</p>
<p>In the practice of dimension reduction, we can implement various methods of regression for highly dimensional data, including PCA regression (regression on principle components).</p>
<p>Investment portfolio example:</p>
<p>Consider a case of stock market investing, where we want to build a diversified investment portfolio. We want to know how many stocks are enough for the portfolio to have diversified away most of the unsystematic (microeconomic – firm oriented) risk. For the sake of this example, we will ignore systematic (macroeconomic) risk that affects all stocks.</p>
<p>In the attempt to avoid risk, suppose the investor purchases hundreds of corporate stocks. However, many of these stocks exhibit similar daily trading behavior. They are generally up or down in tandem in the same time period. In the statistical evaluation of the portfolio, the respective time series data of these stocks will show multicollinearity, such that beyond a certain number of portfolio holdings, there is no reduction in portfolio volatility.</p>
<p>Research indicates that the optimal number of stocks for a portfolio is approximately 16. (See <a href="http://news.morningstar.com/classroom2/course.asp?docId=145385&amp;page=4">http://news.morningstar.com/classroom2/course.asp?docId=145385&amp;page=4</a> ). While other studies conclude that more stocks are needed, the case is clear: multicollinearity among investment time series data indicates that a portfolio can be effectively diversified with fewer holdings.</p>
<p>Related: <a href="https://www.investopedia.com/articles/stocks/11/illusion-of-diversification.asp">https://www.investopedia.com/articles/stocks/11/illusion-of-diversification.asp</a></p>
<p>A small randomly selected portfolio will on average see the same return as the general market, such as represented by the S&amp;P 500 Index. To outperform the market you would need to include at least one non-correlating investment that outperforms the the market.</p>
<p>While high dimensionality in the investment portfolio entails redundant highly correlated investments. I should point out that dimension reduction of such is distinct from the financial concept known as “portfolio optimization.” The latter has to do with finding a portfolio's optimal return/risk combination given various weights of select assets.</p>
<p>Related:</p>
<p><a href="http://www.efficientfrontier.com/ef/900/15st.htm">http://www.efficientfrontier.com/ef/900/15st.htm</a></p>
<p><a href="https://blog.quantopian.com/markowitz-portfolio-optimization-2/">https://blog.quantopian.com/markowitz-portfolio-optimization-2/</a></p>
<p>Sample density:</p>
<p>The fewer the input features (dimensions), the less separable the records. To see why, consider in a single time or date dimension two exchange traded funds representing S&amp;P 500 (SPY) and US Treasury bonds (TLT):</p>
<pre><code>---0.3---0.5---0.7---0.9---        
               SPY  
               TLT
</code></pre>
<p>In this one dimension, a single observation in time, the objects of interest have the same axis point and are, therefore, indistinguishable. Now if we add a second dimension of price, the objects become separable in the 2D space, as seen below.</p>
<pre><code>   -
0.9-           SPY
   -
0.7-
   -           TLT
0.5-
   -
---0.3---0.5---0.7---0.9---        
</code></pre>
<p>Yet, even in two dimensions there are going to be patterns that are not linearly separable. SPY will overlap with the Dow Jones Industrial Average ETF (DIA) on some occasions. Too look for separation, we can add more features, such as trade volume, dividends,...</p>
<p>At this point we may be encouraged to further expand dimensions to linearly separate a tight cluster of data. However, the added feature dimension reduces exponentially the percentage coverage of the feature range. We have created “sparseness” in the feature space, in which population data is not represented.</p>
<p>As we increase dimensions D over I discrete intervals with n data samples, the “sample density” is:</p>
<p>density = n / I<sup>D</sup></p>
<p>For example, consider the density of 8 stocks over 10 time periods in D dimensions:</p>
<table>
<thead>
<tr class="header">
<th>D</th>
<th>n</th>
<th>I</th>
<th>density</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>8</td>
<td>10</td>
<td>0.8</td>
</tr>
<tr class="even">
<td>2</td>
<td>8</td>
<td>10</td>
<td>0.08</td>
</tr>
<tr class="odd">
<td>3</td>
<td>8</td>
<td>10</td>
<td>0.008</td>
</tr>
</tbody>
</table>
<p>This implies that additional dimensions result in each sample having a smaller footprint in the dimensional space.</p>
<p>Thus, to train data to represent 15% of a 1D feature range, for 2D we need a sample that represents 0.15<sup>1/2</sup> or 38.7% of the population. For 3D, we would need 53% of the population. At some point we end up over-fitting a model to 100% of the population data, which makes the model unsuitable for interpreting new data.</p>
<p>A non-linear classifier in fewer dimensions will not necessarily get out of the woods either, as we can still run into over-fitting problems. The classifier may still be dependent on specific data without the flexibility to generalize.</p>
<p>Lets say we go beyond 3D by adding various transformations on the sample data, such as moving averages and lags. We are attempting to differentiate feature vectors that are similar among the sample data. However, when testing with new unforeseen data then model fails. Having been reminded of the sparsity problem from adding dimensions, we instead seek more sample data. The problem is that the population data is too vast or inaccessible. For example, for the purposes of prediction, we do not have the luxury of sampling future data.</p>
<p>A better approach is to use a classifier of relatively few dimensions among uncorrelated data and an acceptable threshold of error.</p>
<p>Geometrically, as dimensions increase, vertices move away from the origin and towards the perimeter of the feature space, making distances between vertices less meaningful. A similar matter is the volume of the hypersphere as it relates to dimensions. More: <a href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">https://en.wikipedia.org/wiki/Volume_of_an_n-ball</a>. <img src="media/nball.png" /> For a given dimension d, consider that the minimum and the maximum distances may become indiscernible such that:</p>
<p>lim d → ∞ E ( dist max ⁡ ( d ) − dist min ⁡ ( d ) dist min ⁡ ( d ) ) → 0</p>
<p>Accordingly, classification methods that rely on distance measures, such as KNN, will be compromised by high dimension data.</p>
<p>More: <a href="https://link.springer.com/chapter/10.1007%2F3-540-49257-7_15">https://link.springer.com/chapter/10.1007%2F3-540-49257-7_15</a></p>
<h2 id="over-fitting-high-dimensional-data">Over-fitting high dimensional data</h2>
<p>Now, instead of having a portfolio of with dimensions comprised of distinct stock holdings, we consider a single stock with dimensions as distinct price patterns in time series data.</p>
<p>It is commonly observed that certain price patterns in time series are indicative of later trends. One such pattern is called a “double bottom” and exhibits a “W” shaped pattern. This pattern tends to precede price increases. A contrary pattern is the double top, which resembles the letter “M” and tends to precede price drops.</p>
<p>The following graphics of W and M patterns are from : <a href="https://www.investopedia.com/university/charts/charts4.asp">https://www.investopedia.com/university/charts/charts4.asp</a></p>
<p><img src="media/Pictures/10000201000003920000022D057E7D63AD323E02.png" /></p>
<p><img src="media/Pictures/100002010000041F0000022A717548A2B41024AA.png" /></p>
<p>We can affect machine learning to recognize such patterns. To do so, we need to create a classifier that corresponds to the defining points of a pattern. For example, both the M and W patterns can be minimally represented with just five points on an XY (price x time) plane.</p>
<p>With standardized price and time data, each vertex of the pattern W can be represented by (X,Y) in centile units, from 0.01 to 1. So, each price data point P is standardized as a percentage of the range between the maximum and minimum prices in the sample, and any given date D is described as a percentage of elapsed dates in the sample. (If the data is erratic, it might first be smoothed using a moving average.) Then, each of the five pattern points for one particular shape of W and scale looks like:</p>
<p>Yi = (Pi - Pmin) / (Pmax - Pmin)</p>
<p>Xi = Di / Dn</p>
<p>To train a classifier for M and W, we can use a variety of sample W and M patterns. For example, the W pattern could be the average of hundreds of variously shaped W patterns.</p>
<p>The basic pattern descriptor applicable to both M and W pattern classes:</p>
<p>Pattern = { (X1, Y1), (X2, Y2), (X3, Y3), (X4, Y4), (X5, Y5) }</p>
<p>The condition for pattern W is:</p>
<p>if(Y2 &lt; Y1 AND Y3 &gt; Y2 AND Y4 &lt; Y3 AND Y5 &gt; Y4): W</p>
<p>The condition for pattern M is just the opposite:</p>
<p>if(Y2 &gt; Y1 AND Y3 &lt; Y2 AND Y4 &gt; Y3 AND Y5 &lt; Y4): M</p>
<p>Of course, these simplistic conditions are bound to be inaccurate without accommodating a certain deviation from each point. For example:</p>
<p>if((Y2 &gt; Y1 AND Y3 &lt; Y2 AND Y4 &gt; Y3 AND Y5 &lt; Y4) <strong>OR</strong></p>
<p>((Y2 + 0.01) &gt; Y1 AND Y3 &lt; (Y2 + 0.01) AND (Y4 + 0.01) &gt; Y3 AND Y5 &lt; (Y4 + 0.01))): M</p>
<p>We could also included a scaling factor for accommodating the patterns at various scales.</p>
<p>Testing the 5 points with 10 shape variations and 10 scales would make for 5 x 10 10 or 500 feature combinations per time series window per stock symbol data set. Provided the pattern's scale covers 28 trading days, a year's worth of times series data (280 business days) entails 14,000 point comparisons, for just that one scale. With all 10 scales and several years of training data, classifier training could require some patience.</p>
<p>Generalization versus over-fitting:</p>
<p>By allowing the classifier to match general features or pattern variations, its can match unforeseen data. On the other hand, if we try to train the classifier for any conceivable data point, we are really just implementing a system of pattern matching that is no different than a look-up table. Such a system will not work for prediction since future price patterns have not yet been recorded in the look-up table. This is a case of over-fitting, as the classifier is unable to generalize about unforeseen data.</p>
<p>While the optimal number of dimensions depends on the problem at hand, the general relationship between classifier performance and dimensions will be a curve skewed to the left.</p>
<p><img src="media/leftskew.png" /></p>
<h2 id="detecting-the-curse-of-dimensionality">Detecting the curse of dimensionality</h2>
<ul>
<li><p>Examine regression models have a good fit. Good forecasts do not necessarily come from models having a good fit.</p></li>
<li><p>Removing some variables reduces regression standard errors.</p></li>
<li><p>Variable pairs generally show strong correlation.</p></li>
<li><p>Similarly, one independent variable exhibits a high r-squared with other independent variables.</p></li>
<li><p>Parameter estimates vary significantly between sample data sets.</p></li>
<li><p>Standard errors on regression coefficients.</p></li>
<li><p>Where there is multicollinearity and no pairwise correlations are high, there may be joint dependencies that are multi-collinear. Testing for such requires exponentially more iterations as we additionally test for joint dependencies for each independent variable pair.</p></li>
<li><p>Test forecast efficacy having removed independent variables that are weakly correlated to the dependent. This will at least clear away some redundancy.</p></li>
<li><p>Consider Variance Inflation Factors (See <a href="https://cran.r-project.org/web/packages/olsrr/vignettes/regression_diagnostics.html">https://cran.r-project.org/web/packages/olsrr/vignettes/regression_diagnostics.html</a> )</p></li>
</ul>
<h2 id="remedying-the-curse-of-dimensionality">Remedying the curse of dimensionality</h2>
<p>Let this this section be a segue to the Dimension Reduction module. A few general solutions to reduce dimensionality as well as the variance of regression coefficients include:</p>
<ul>
<li><p>Make sure dimensions do not outnumber observations.</p></li>
<li><p>Employ dimension reduction techniques, such as PCA and SOM.</p></li>
<li><p>Employ feature selection algorithms.</p></li>
<li><p>Beware classifiers that over-fit the data.</p></li>
<li><p>Cross validate out of sample data.</p></li>
<li><p>Increase sample size to reduce MSRE.</p></li>
<li><p>Revise the model to:</p>
<ul>
<li>Use data that is logically more relevant.</li>
<li>Look for categorically illogical variables to drop.</li>
<li>Use other data transformations.</li>
</ul></li>
</ul>
<p>Principal Component Analysis (PCA) aims to produce a subset of uncorrelated linearly separable features from the original feature matrix. The resulting sub-feature set may or may not be optimal for classification.</p>
<h3 id="other-internet-resources">Other Internet resources</h3>
<p><a href="http://rsta.royalsocietypublishing.org/content/367/1906/4237">http://rsta.royalsocietypublishing.org/content/367/1906/4237</a></p>
<p><a href="http://featureselection.asu.edu/algorithms.php">http://featureselection.asu.edu/algorithms.php</a></p>
<p><a href="https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335">https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335</a></p>
<h2 id="exercises">Exercises</h2>
<p>In these exercises we will test for the curse of dimensionality.</p>
<p>On web sites such as Google Finance or Yahoo Finance, you can download the daily closing prices for most stock symbols. Download one year of data for ten symbols of your choosing. Additionally, download data for the symbol SPY. Save each download in a separate Cmma Separated Value (.csv) file.</p>
<p>Using the Python Programming language and your stock download files, compete the following tasks:</p>
<ol>
<li><p>Standardize all data to values between 0 and 1 for the dates downloaded.</p></li>
<li><p>Create a correlation matrix for the data of the eleven symbols. Are there clues to multicollinearity in the correlation matrix?</p></li>
<li><p>Compute pair-wise r-squared among the independent variables. Does your correlation matrix corroborate these coefficients?</p></li>
<li><p>Assume that each symbol other than SPY is a dimension of SPY. Using SPY as the dependent variable, compute multivariate regression parameters from the other (independent) symbols (variables). Redo the regression using the five least correlated symbols. Then repeat this again with the five most correlated symbols. What do can you deduce about these three regression models?</p></li>
<li><p>Assume that each symbol other than SPY is a dimension of SPY. Generate the average distance between each feature vector (record at each date). Next generate the average distance between each feature vector using the five least correlated symbols. Repeat this with the five most correlated symbols. What do can you deduce about these three distance totals?</p></li>
</ol>
<p>Upload your code, question responses and any supporting documentation.</p>
<p><a href="https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335"></a></p>

</div>
</div>
</body>
</html>